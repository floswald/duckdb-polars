[
  {
    "objectID": "duckdb-sql.html",
    "href": "duckdb-sql.html",
    "title": "DuckDB SQL",
    "section": "",
    "text": "You can think of a database as a collection of tables (or dataframes). In general, the benefit of databases is that they can handle very large amounts of data - data which would not fit in your computer’s RAM. As such, most databases are stored on a hard disk, from where data is read and written to.\nThere are many different forms of databases. We have several so-called database management systems (DBMS), which allow us to interact with each of them. Unhelpfully, each different flavor of database has a slightly different dialect of a what is called the structured query language, SQL for short. Helpfully, on the other hand, there are several packages out there which help us keeping the effort manageable by translating our queries. We will see a few of those here, and in different driving languages (i.e. R, python and julia).\nOne final important feature of different databases is how (and where) they actually run. The more traditional solutions exist as server-client models, where you would start on a machine (could be your own) a database server, which interacts with the persistent storage (e.g. with your hard disk) to read and write data, and a client process, where you formulate so-called queries, and get back the results from the server. In some sense you formulate a question for the database in the client, you submit it to the database, and you get back an answer. Of course those server processes can run on dedicated and powerful machines which you can reach via a network, or directly in the cloud, where they are used to work with arbitrarily huge amounts of data.\nWe will focus on a relatively recent addition to the database family called duckdb. The main benefit (beyond it’s excellent performance) is that it is very easy to run a database without the need to set up the client-server structure. We will come to that in due course.\n\n\nAs already mentioned, this is the main database language. Here is an example:\nSELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\nThis reads:\n\nFrom the table called diamonds take the columns carat, cut, clarity, color, price, but keep only rows where the column price is larger than the value 15000\n\nNot too bad, right?\nLet’s get this example running now in our computers. We need to create a duckdb database first, read some data into it, and the confront it with that SQL query. For simplicity we will do this only in R to get us started, then we will move on to a more elaborate example and big data, which will use both R, python and julia.",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#what-is-a-database",
    "href": "duckdb-sql.html#what-is-a-database",
    "title": "DuckDB SQL",
    "section": "",
    "text": "You can think of a database as a collection of tables (or dataframes). In general, the benefit of databases is that they can handle very large amounts of data - data which would not fit in your computer’s RAM. As such, most databases are stored on a hard disk, from where data is read and written to.\nThere are many different forms of databases. We have several so-called database management systems (DBMS), which allow us to interact with each of them. Unhelpfully, each different flavor of database has a slightly different dialect of a what is called the structured query language, SQL for short. Helpfully, on the other hand, there are several packages out there which help us keeping the effort manageable by translating our queries. We will see a few of those here, and in different driving languages (i.e. R, python and julia).\nOne final important feature of different databases is how (and where) they actually run. The more traditional solutions exist as server-client models, where you would start on a machine (could be your own) a database server, which interacts with the persistent storage (e.g. with your hard disk) to read and write data, and a client process, where you formulate so-called queries, and get back the results from the server. In some sense you formulate a question for the database in the client, you submit it to the database, and you get back an answer. Of course those server processes can run on dedicated and powerful machines which you can reach via a network, or directly in the cloud, where they are used to work with arbitrarily huge amounts of data.\nWe will focus on a relatively recent addition to the database family called duckdb. The main benefit (beyond it’s excellent performance) is that it is very easy to run a database without the need to set up the client-server structure. We will come to that in due course.\n\n\nAs already mentioned, this is the main database language. Here is an example:\nSELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\nThis reads:\n\nFrom the table called diamonds take the columns carat, cut, clarity, color, price, but keep only rows where the column price is larger than the value 15000\n\nNot too bad, right?\nLet’s get this example running now in our computers. We need to create a duckdb database first, read some data into it, and the confront it with that SQL query. For simplicity we will do this only in R to get us started, then we will move on to a more elaborate example and big data, which will use both R, python and julia.",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#simple-intro-example-with-r",
    "href": "duckdb-sql.html#simple-intro-example-with-r",
    "title": "DuckDB SQL",
    "section": "Simple Intro Example with R",
    "text": "Simple Intro Example with R\n\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(tibble)\ncon = DBI::dbConnect(duckdb::duckdb(), shutdown = TRUE)  # will get erased after shutdown\n# con = DBI::dbConnect(duckdb::duckdb(), dbdir = \"db.duck\") # would save on disk\n\nThe con object is our live connection to the database. Think of it like a telephone receiver - you can talk into it, and it will respond results back to you. As such, we have to refer to con in everything we want to do with the database.\nLet us first read some data from the ggplot2 package, and add it to our database. Remember, a database is a collection of tables, and now we are adding two of those:\n\ndbWriteTable(con, \"mpg\", ggplot2::mpg)\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\n\nOk, let’s see what tables we have in the database now:\n\ndbListTables(con)\n\n[1] \"diamonds\" \"mpg\"     \n\n\nGreat. Now how do get data out of it? Here goes the mpg dataset:\n\ncon |&gt;\n    dbReadTable(\"mpg\") |&gt;\n    as_tibble()\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nBack to our SQL query from above. We can submit it to the database listening at con:\n\nquery1 = \"SELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\"\n\nhead(dbGetQuery(con, query1))\n\n  carat       cut clarity color price\n1  1.54   Premium     VS2     E 15002\n2  1.19     Ideal    VVS1     F 15005\n3  2.10   Premium     SI1     I 15007\n4  1.69     Ideal     SI1     D 15011\n5  1.50 Very Good    VVS2     G 15013\n6  1.73 Very Good     VS1     G 15014\n\n\nor of course as before with a pipeline such as\n\ncon |&gt;\n    dbGetQuery(query1) |&gt;\n    as_tibble()\n\n# A tibble: 1,655 × 5\n   carat cut       clarity color price\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt; &lt;int&gt;\n 1  1.54 Premium   VS2     E     15002\n 2  1.19 Ideal     VVS1    F     15005\n 3  2.1  Premium   SI1     I     15007\n 4  1.69 Ideal     SI1     D     15011\n 5  1.5  Very Good VVS2    G     15013\n 6  1.73 Very Good VS1     G     15014\n 7  2.02 Premium   SI2     G     15014\n 8  2.05 Very Good SI2     F     15017\n 9  1.5  Very Good VS1     F     15022\n10  1.82 Very Good SI1     G     15025\n# ℹ 1,645 more rows\n\n\nThat’s it for starters. There is now a whole world out there to learn in terms of SQL queries, database joins, pivots (i.e. reshape operations) etc. I leave you in the hands of Hadley Wickham and his dedicated chapter on this topic if you want to dive deeper. It will be almost unavoidable to learn some simple SQL, but hopefully it won’t be so bad. You will see in the associated pages of this tutorial that there are some very nice translation tools for R. Grant’s intro to databases is another valuable resource here. Now, however, let’s get cracking with the real show, i.e. our big data set on NYC taxi rides.\n\n# setup python from R with reticulate package\nlibrary(reticulate)\nSys.setenv(RETICULATE_PYTHON=here::here(\".venv/bin/python\"))",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#load-libraries",
    "href": "duckdb-sql.html#load-libraries",
    "title": "DuckDB SQL",
    "section": "Load libraries",
    "text": "Load libraries\n\nRPython\n\n\n\nlibrary(duckdb)  # loaded already \n\nLoading required package: DBI\n\n\n\n\n\nimport duckdb\nimport time # just for timing some queries",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#create-a-database-connection",
    "href": "duckdb-sql.html#create-a-database-connection",
    "title": "DuckDB SQL",
    "section": "Create a database connection",
    "text": "Create a database connection\nThe first thing we need to do is instantiate a connection with an in-memory database.1\n\nRPython\n\n\n\ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n\n\n\ncon = duckdb.connect(database = ':memory:', read_only = False)\n\n\n\n\n\nDigression: In-memory versus on-disk\nThe fact that our connection lives “in memory” is a killer feature of DuckDB (one that it inherits from SQLite). We don’t need to connect to some complicated, existing database infrastructure to harness all of DuckDB’s power. Instead we can just spin up an ephemeral database that interacts directly with our R (or Python, or Julia, etc.) client.\nHowever, it’s worth noting that you can create a persistent, disk-backed database simply by providing a database file path argument as part of your connection, e.g.\n\nRPython\n\n\n## Uncomment and run the next line if you'd like to create a persistent,\n## disk-backed database instead.\n\n# con = dbConnect(duckdb(), dbdir = \"nyc.duck\")\n\n\n## Uncomment and run the next line if you'd like to create a persistent,\n## disk-backed database instead.\n\n# con = duckdb.connect(database = 'nyc.duck', read_only = False)\n\n\n\n(Note that the \".duck\" file extension above is optional. You could also use \".db\", \".dbb\", or whatever you want really.)\n\n\n\n\n\n\nBigger than RAM data?\n\n\n\nOne really important benefit of creating a persistent disk-backed database is that it enables out-of-core computation for bigger than RAM data. See here for more details and performance considerations (which are still great).",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#just-how-big-is-this",
    "href": "duckdb-sql.html#just-how-big-is-this",
    "title": "DuckDB SQL",
    "section": "Just How Big is This?",
    "text": "Just How Big is This?\nBefore we get started with some analysis tasks, here’s a simple question to ask our database: how many rows are we dealing with here, actually?\n\nRpython\n\n\n\nnrows = dbGetQuery(\n      con,\n      \"\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        COUNT(passenger_count) as nrows\n      \"\n    )\n\n\n\n\n# con.\n#   query(\n#   '''\n#     FROM 'nyc-taxi/**/*.parquet'\n#     SELECT\n#     COUNT(passenger_count) as nrows\n#   '''\n# )\n\n\n\n\nOk, well that is 178,544,324. 😬",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#first-example",
    "href": "duckdb-sql.html#first-example",
    "title": "DuckDB SQL",
    "section": "First example",
    "text": "First example\nWe’ll start with a simple aggregation query to get situated. I’ll also use this example to highlight some general features of DuckDB SQL and the underlying query engine.\nOkay, first query. Let’s say we want to know: What is the average tip per passenger count? A typical SQL job for this question might look as follows:\nSELECT\n  passenger_count,\n  AVG(tip_amount) AS mean_tip\nFROM 'nyc-taxi/**/*.parquet'\nGROUP BY passenger_count\nORDER BY passenger_count\n(Where the last ORDER BY statement is optional. Note that ordering (i.e., sorting) is a potentially expensive operation but we’ll get back to that later.)\nThis is perfectly valid DuckDB SQL too. However, we can rewrite it with slightly nicer syntax thanks DuckDB’s “friendly SQL”. The key changes for this simple query are going to be: (1) the FROM statement comes first, and (2) we can use the GROUP BY ALL and ORDER BY ALL statements to avoid writing out the “passenger_count” grouping column multiple times.2\nFROM 'nyc-taxi/**/*.parquet'\nSELECT\n  passenger_count,\n  AVG(tip_amount) AS mean_tip\nGROUP BY ALL\nORDER BY ALL\n\n\n\n\n\n\nDuckDB’s “friendly SQL”\n\n\n\nOne of the under-appreciated (IMHO) features of DuckDB is that it supports many syntax enhancements over tradional SQL dialects, which they collectively dub “friendly SQL”. Together these syntax enhancements allow you to write much more ergonomic SQL queries that cut down on duplication and logical inconsistencies.\n\n\nTo run this operation from our R or Python client, simply pass the SQL query as a string to our connection. Let’s use this as a chance to save the result and time our query too.\n\nRPython\n\n\n\ntic = Sys.time()\ndat1 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  \"\n)\ntoc = Sys.time()\n\ndat1\n\n   passenger_count  mean_tip\n1                0 0.8620988\n2                1 1.1510110\n3                2 1.0815798\n4                3 0.9629494\n5                4 0.8445190\n6                5 1.1027325\n7                6 1.1283649\n8                7 0.5441176\n9                8 0.3507692\n10               9 0.8068000\n11              10 0.0000000\n12              65 0.0000000\n13              66 1.5000000\n14             177 1.0000000\n15             208 0.0000000\n16             247 2.3000000\n17             249 0.0000000\n18             254 0.0000000\n\ntoc - tic\n\nTime difference of 2.233316 secs\n\n\n\n\n\ntic = time.time()\ndat1 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    GROUP BY ALL\n    ORDER BY ALL\n    '''\n    )\n)\ntoc = time.time()\n\ndat1\n\n┌─────────────────┬─────────────────────┐\n│ passenger_count │      mean_tip       │\n│      int64      │       double        │\n├─────────────────┼─────────────────────┤\n│               0 │   0.862098814142438 │\n│               1 │  1.1510109615467636 │\n│               2 │  1.0815798424002565 │\n│               3 │   0.962949365789299 │\n│               4 │  0.8445189789660231 │\n│               5 │  1.1027324532618157 │\n│               6 │  1.1283649236954085 │\n│               7 │  0.5441176470588235 │\n│               8 │ 0.35076923076923083 │\n│               9 │              0.8068 │\n│              10 │                 0.0 │\n│              65 │                 0.0 │\n│              66 │                 1.5 │\n│             177 │                 1.0 │\n│             208 │                 0.0 │\n│             247 │                 2.3 │\n│             249 │                 0.0 │\n│             254 │                 0.0 │\n├─────────────────┴─────────────────────┤\n│ 18 rows                     2 columns │\n└───────────────────────────────────────┘\n\n# print(f\"Time difference of {toc - tic} seconds\")\n## Timing will be misleading for this rendered Quarto doc, since we're calling\n## Python from R (via the reticulate package).\n\nNote that we actually get a polars DataFrame as a return object. Click the callout box below to learn more.\n\n\n\n\n\n\nResult conversion in Python (click to expand)\n\n\n\n\n\nBy default, the con.query method that we are using here will return a polars DataFrame object that Python understands “natively” (i.e., has a print method for and so on). Behind the scenes, this duckdb to polars integration relies on the pyarrow library being available to our Python environment, which have already installed for this workshop.\nIt’s also possible return other types of Python objects. For example, you can use the .df() method to coerce to a pandas DataFrame instead, among several other formats like numpy arrays. (Details here.) Given the focus of this workshop, it won’t surprise you to hear that I’m going to stick with the default polars conversion.\n\n\n\n\n\n\nSo that only took 2.23 seconds in this rendered Quarto doc (and will likely be even faster when you try in an interactive session). To underscore just how crazy impressive this is, recall that this includes the time that it takes to read the data from disk. I can almost guarantee that the read + serialization time alone for traditional data wrangling workflows would take several minutes, and probably crash my laptop RAM. Don’t forget that our full dataset is nearly 200 million rows deep and 30 columns wide.\nAside: We clearly have a few outlier typos in our dataset. 254 passengers in a single taxi trip? I don’t think so. We’d probably want to filter these out with a WHERE statement if we were doing serious analysis, but I’m just going to leave them in for this tutorial.",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#aggregation",
    "href": "duckdb-sql.html#aggregation",
    "title": "DuckDB SQL",
    "section": "Aggregation",
    "text": "Aggregation\nLet’s try out some more aggregation queries. How about a slightly variation on a our first example query, where we (a) add “month” as a second grouping variable, and (b) subset to only the first three months of the year.\n\nRPython\n\n\n\ntic = Sys.time()\ndat2 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    month,\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  WHERE month &lt;= 3\n  GROUP BY ALL\n  \"\n    )\ntoc = Sys.time()\n\nhead(dat2)\n\n  month passenger_count  mean_tip\n1     1               3 0.8752660\n2     1               6 1.0175694\n3     2               3 0.8948977\n4     2               6 1.0218459\n5     3               6 1.0515659\n6     3               3 0.9121819\n\ntoc - tic\n\nTime difference of 0.585984 secs\n\n\n\n\n\ntic = time.time()\ndat2 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n    '''\n  )\n)\ntoc = time.time()\n\ndat2\n\n┌───────┬─────────────────┬────────────────────┐\n│ month │ passenger_count │      mean_tip      │\n│ int64 │      int64      │       double       │\n├───────┼─────────────────┼────────────────────┤\n│     1 │               3 │ 0.8752659692390745 │\n│     1 │               6 │ 1.0175694433120157 │\n│     2 │               3 │ 0.8948976704752726 │\n│     2 │               6 │ 1.0218459360559868 │\n│     3 │               3 │ 0.9121818858010095 │\n│     3 │               6 │ 1.0515659390082799 │\n│     1 │               0 │ 0.8417178636744063 │\n│     2 │               0 │ 0.8766365326038179 │\n│     3 │               0 │ 0.8776747542704466 │\n│     1 │               5 │ 1.0011984858733018 │\n│     · │               · │                 ·  │\n│     · │               · │                 ·  │\n│     · │               · │                 ·  │\n│     1 │               7 │                0.0 │\n│     1 │               8 │                0.0 │\n│     1 │               1 │  1.036862814231083 │\n│     2 │               1 │ 1.0684897865838967 │\n│     3 │               1 │ 1.0892047410487358 │\n│     1 │               2 │ 0.9647090920018082 │\n│     1 │             208 │                0.0 │\n│     2 │               2 │ 0.9908003546925659 │\n│     3 │               2 │ 1.0096468528132239 │\n│     3 │             208 │                0.0 │\n├───────┴─────────────────┴────────────────────┤\n│ 29 rows (20 shown)                 3 columns │\n└──────────────────────────────────────────────┘\n\n# print(f\"Time difference of {toc - tic} seconds\")\n## Timing will be misleading for this rendered Quarto doc, since we're calling\n## Python from R (via the reticulate package).\n\n\n\n\nNote that this query completed even faster than the first one, even though we added another grouping variable. Reason: Subsetting along our Hive-partitioned parquet dataset allows DuckDB to take shortcuts. We can see this directly by prepending an EXPLAIN statement to our query to reveal the optmized query plan.\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  EXPLAIN\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n  \"\n)\n\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│    ────────────────────   │\n│          Groups:          │\n│             #0            │\n│             #1            │\n│                           │\n│    Aggregates: avg(#2)    │\n│                           │\n│       ~22453698 Rows      │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│    ────────────────────   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│                           │\n│       ~44907396 Rows      │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│       PARQUET_SCAN        │\n│    ────────────────────   │\n│         Function:         │\n│        PARQUET_SCAN       │\n│                           │\n│        Projections:       │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│                           │\n│       File Filters:       │\n│        (month &lt;= 3)       │\n│                           │\n│    Scanning Files: 3/12   │\n│                           │\n│       ~44907396 Rows      │\n└───────────────────────────┘\n\n\n\n\n\ncon.query(\n  '''\n  EXPLAIN\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n  '''\n)\n\n┌───────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│  explain_key  │                                            explain_value                                             │\n│    varchar    │                                               varchar                                                │\n├───────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ physical_plan │ ┌───────────────────────────┐\\n│       HASH_GROUP_BY       │\\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\\n│    …  │\n└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\ntl;dr DuckDB is able to exploit the month partition of our dataset, so subsetting means that it can avoid unecessary data ingestion. Similarly, it only reads in a select group of columns; that’s what the “PROJECTION” part of the plan denotes. If nothing else, the take-home message is that DuckDB only does what it needs to. Laziness as a virtue!\nHere’s a final aggregation example, this time including a high-dimensional grouping column (i.e., “trip_distance”) and some additional aggregations.\n\nRPython\n\n\n\ntic = Sys.time()\ndat3 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    passenger_count,\n    trip_distance,\n    AVG(tip_amount) AS mean_tip,\n    AVG(fare_amount) AS mean_fare\n  GROUP BY ALL\n\"\n)\ntoc = Sys.time()\n\nnrow(dat3)\n\n[1] 25569\n\nhead(dat3)\n\n  passenger_count trip_distance  mean_tip mean_fare\n1               2           2.6 0.9732562 10.619201\n2               1           2.6 1.1313478 10.543915\n3               1           6.4 2.0454613 19.462782\n4               0           4.1 1.3378420 13.334745\n5               0           1.3 0.6467107  6.559703\n6               1           9.5 3.4773251 26.061723\n\ntoc - tic\n\nTime difference of 4.049236 secs\n\n\n\n\n\ntic = time.time()\ndat3 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      trip_distance,\n      AVG(tip_amount) AS mean_tip,\n      AVG(fare_amount) AS mean_fare\n    GROUP BY ALL\n    '''\n    )\n)\ntoc = time.time()\n\nlen(dat3)\n\n25569\n\ndat3\n\n┌─────────────────┬───────────────┬─────────────────────┬────────────────────┐\n│ passenger_count │ trip_distance │      mean_tip       │     mean_fare      │\n│      int64      │    double     │       double        │       double       │\n├─────────────────┼───────────────┼─────────────────────┼────────────────────┤\n│               1 │          10.6 │   3.798417230590116 │  28.70962220766925 │\n│               4 │           0.8 │ 0.31864535306162817 │  5.521868287205346 │\n│               1 │           3.1 │  1.2789624509439401 │ 11.872071130994996 │\n│               1 │           5.3 │   1.812836031545094 │ 17.073445743534876 │\n│               2 │          16.1 │   4.069615695792877 │  45.16434061488674 │\n│               1 │           2.4 │  1.0694859422619258 │ 10.005680611735821 │\n│               2 │          11.4 │  3.1219094019637015 │  31.17485569770902 │\n│               4 │           1.0 │ 0.35807810956799535 │   6.19363893529085 │\n│               3 │           5.3 │  1.3933198870512298 │   17.3719846712384 │\n│               2 │           3.1 │   1.115457631079296 │ 11.991438118884266 │\n│               · │            ·  │           ·         │          ·         │\n│               · │            ·  │           ·         │          ·         │\n│               · │            ·  │           ·         │          ·         │\n│               2 │          3.49 │  1.2619583410823876 │ 12.730611865352428 │\n│               6 │         16.97 │  4.9274531835205995 │  47.52022471910112 │\n│               2 │         11.62 │   3.246236842105263 │ 31.140263157894736 │\n│               4 │          10.7 │  2.0272682201289043 │ 29.590927119484384 │\n│               1 │         20.72 │   5.406076260762608 │  49.67367773677737 │\n│               4 │          1.02 │   0.512334250343879 │  5.957606602475937 │\n│               6 │          7.31 │   2.157258741258741 │ 21.743776223776223 │\n│               3 │          11.8 │  2.6827715064227324 │ 32.187855196574546 │\n│               1 │         14.27 │   3.765272887323943 │ 39.136839788732395 │\n│               2 │          5.26 │  1.6736970837253058 │  16.93365475070555 │\n├─────────────────┴───────────────┴─────────────────────┴────────────────────┤\n│ ? rows (&gt;9999 rows, 20 shown)                                    4 columns │\n└────────────────────────────────────────────────────────────────────────────┘\n\n# print(f\"Time difference of {toc - tic} seconds\")\n## Timing will be misleading for this rendered Quarto doc, since we're calling\n## Python from R (via the reticulate package).",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#pivot-reshape",
    "href": "duckdb-sql.html#pivot-reshape",
    "title": "DuckDB SQL",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\nLet’s explore some pivot (reshape) examples, by building off the previous query.\n\nUNPIVOT: wide =&gt; long\nPIVOT: long =&gt; wide\n\nHere I’ll use a Common Table Expression (CTE) to define a temporary table tmp_table, before unpivoting—i.e., reshaping long—at the end.\n\nRPython\n\n\n\ndat_long = dbGetQuery(\n  con,\n  \"\n  WITH tmp_table AS (\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      trip_distance,\n      AVG(tip_amount) AS mean_tip,\n      AVG(fare_amount) AS mean_fare\n    GROUP BY ALL\n  )\n  UNPIVOT tmp_table\n  ON mean_tip, mean_fare\n  INTO\n    NAME variable\n    VALUE amount\n  \"\n)\n\nhead(dat_long)\n\n  passenger_count trip_distance  variable    amount\n1               1           0.8  mean_tip 0.5347135\n2               1           0.8 mean_fare 5.3604431\n3               1           1.0  mean_tip 0.6155292\n4               1           1.0 mean_fare 6.0121158\n5               1           0.4  mean_tip 0.3612548\n6               1           0.4 mean_fare 4.1737650\n\n\n\n\n\ndat_long = (\n  con.\n  query(\n    '''\n    WITH tmp_table AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        passenger_count,\n        trip_distance,\n        AVG(tip_amount) AS mean_tip,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY ALL\n    )\n    UNPIVOT tmp_table\n    ON mean_tip, mean_fare\n    INTO\n      NAME variable\n      VALUE amount\n    '''\n  )\n)\n\ndat_long\n\n┌─────────────────┬───────────────┬───────────┬────────────────────┐\n│ passenger_count │ trip_distance │ variable  │       amount       │\n│      int64      │    double     │  varchar  │       double       │\n├─────────────────┼───────────────┼───────────┼────────────────────┤\n│               2 │           1.8 │ mean_tip  │ 0.7537931462210244 │\n│               2 │           1.8 │ mean_fare │  8.450205623706095 │\n│               1 │           0.2 │ mean_tip  │ 0.3864478915075298 │\n│               1 │           0.2 │ mean_fare │   4.67119257714553 │\n│               1 │          3.47 │ mean_tip  │ 1.3073502150223362 │\n│               1 │          3.47 │ mean_fare │  12.75832115569286 │\n│               1 │           4.7 │ mean_tip  │ 1.6794478064449927 │\n│               1 │           4.7 │ mean_fare │ 15.728522049844548 │\n│               1 │           2.7 │ mean_tip  │ 1.1625732397772206 │\n│               1 │           2.7 │ mean_fare │ 10.811487937005014 │\n│               · │            ·  │    ·      │          ·         │\n│               · │            ·  │    ·      │          ·         │\n│               · │            ·  │    ·      │          ·         │\n│               6 │          6.06 │ mean_tip  │ 1.7787407407407405 │\n│               6 │          6.06 │ mean_fare │ 18.787736625514402 │\n│               3 │          8.82 │ mean_tip  │ 2.5983870967741933 │\n│               3 │          8.82 │ mean_fare │ 24.434677419354845 │\n│               4 │          8.14 │ mean_tip  │ 2.4210869565217386 │\n│               4 │          8.14 │ mean_fare │ 23.169130434782605 │\n│               2 │          8.58 │ mean_tip  │   2.58382030679328 │\n│               2 │          8.58 │ mean_fare │ 23.940832724616502 │\n│               5 │          6.58 │ mean_tip  │ 1.9479097682119209 │\n│               5 │          6.58 │ mean_fare │  19.83091887417219 │\n├─────────────────┴───────────────┴───────────┴────────────────────┤\n│ ? rows (&gt;9999 rows, 20 shown)                          4 columns │\n└──────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nAnother option would have been to create a new table in memory and then pivot over that, which segues nicely to…\n\nDigression: Create new tables\nCTEs are a very common, and often efficient, way to implement multi-table operations in SQL. But, for the record, we can create new tables in DuckDB’s memory cache pretty easily using the CREATE TABLE statement.\n\nRPython\n\n\nInstead of DBI::dbGetQuery, we must now use DBI::dbExecute.\n\ndbExecute(\n    con,\n    \"\n    CREATE TABLE taxi2 AS\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        passenger_count,\n        trip_distance,\n        AVG(tip_amount) AS mean_tip,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY ALL\n    \"\n)\n\n[1] 25569\n\ndbListTables(con)\n\n[1] \"taxi2\"\n\n\nFWIW, you can always remove a table with dbRemoveTable().\n\n\nInstead of con.query(), we must now use con.execute().\n\ncon.execute(\n  '''\n  CREATE TABLE taxi2 AS\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      trip_distance,\n      AVG(tip_amount) AS mean_tip,\n      AVG(fare_amount) AS mean_fare\n    GROUP BY ALL\n  '''\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x127f52e30&gt;\n\n# https://stackoverflow.com/q/75727685\ncon.query(\n  '''\n  SELECT table_name, estimated_size AS nrows, column_count AS ncols\n  FROM duckdb_tables;\n  '''\n)\n\n┌────────────┬───────┬───────┐\n│ table_name │ nrows │ ncols │\n│  varchar   │ int64 │ int64 │\n├────────────┼───────┼───────┤\n│ taxi2      │ 25569 │     4 │\n└────────────┴───────┴───────┘\n\n\n\n\n\n\n\nBack to reshaping\nWith our new taxi2 table in hand, let’s redo the previous unpivot query directly on this new table. This makes the actual (un)pivot statement a bit clearer… and also separates out the execution time.\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  UNPIVOT taxi2\n  ON mean_tip, mean_fare\n  INTO\n    NAME variable\n    VALUE amount\n  LIMIT 5\n  \"\n)\n\n  passenger_count trip_distance  variable     amount\n1               1           2.0  mean_tip  0.9423356\n2               1           2.0 mean_fare  8.9324987\n3               1          10.8  mean_tip  3.8087776\n4               1          10.8 mean_fare 29.2385560\n5               1           3.0  mean_tip  1.2506863\n\n\n\n\n\ncon.query(\n  '''\n  UNPIVOT taxi2\n  ON mean_tip, mean_fare\n  INTO\n    NAME variable\n    VALUE amount\n  LIMIT 5\n  '''\n)\n\n┌─────────────────┬───────────────┬───────────┬────────────────────┐\n│ passenger_count │ trip_distance │ variable  │       amount       │\n│      int64      │    double     │  varchar  │       double       │\n├─────────────────┼───────────────┼───────────┼────────────────────┤\n│               1 │           3.7 │ mean_tip  │ 1.4340712759977325 │\n│               1 │           3.7 │ mean_fare │ 13.385660725559163 │\n│               1 │           2.9 │ mean_tip  │ 1.2233798516044125 │\n│               1 │           2.9 │ mean_fare │ 11.352554762180501 │\n│               2 │           4.0 │ mean_tip  │ 1.3716844336946659 │\n└─────────────────┴───────────────┴───────────┴────────────────────┘\n\n\n\n\n\n(Note how crazy fast pivoting in DuckDB actually is.)",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#joins-merges",
    "href": "duckdb-sql.html#joins-merges",
    "title": "DuckDB SQL",
    "section": "Joins (merges)",
    "text": "Joins (merges)\nIt’s a bit hard to demonstrate a join with only a single main table. But here is a contrived example, where we calculate the mean monthly tips and the mean monthly fares as separate sub-queries (CTEs), before joining them together by month.\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  WITH \n    mean_tips AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(tip_amount) AS mean_tip\n      GROUP BY month\n    ),\n    mean_fares AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY month \n    )\n  FROM mean_tips\n  LEFT JOIN mean_fares\n  USING (month)\n  SELECT *\n  ORDER BY mean_tips.month\n  \"\n)\n\n   month mean_tip mean_fare\n1      1 1.007817  9.813488\n2      2 1.036874  9.942640\n3      3 1.056353 10.223107\n4      4 1.043167 10.335490\n5      5 1.078014 10.585157\n6      6 1.091082 10.548651\n7      7 1.059312 10.379943\n8      8 1.079521 10.492650\n9      9 1.254601 12.391198\n10    10 1.281239 12.501252\n11    11 1.250903 12.270138\n12    12 1.237651 12.313953\n\n\n\n\n\ncon.query(\n  '''\n  WITH \n    mean_tips AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(tip_amount) AS mean_tip\n      GROUP BY month\n    ),\n    mean_fares AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY month \n    )\n  FROM mean_tips\n  LEFT JOIN mean_fares\n  USING (month)\n  SELECT *\n  ORDER BY mean_tips.month\n  '''\n)\n\n┌───────┬────────────────────┬────────────────────┐\n│ month │      mean_tip      │     mean_fare      │\n│ int64 │       double       │       double       │\n├───────┼────────────────────┼────────────────────┤\n│     1 │ 1.0078165246989736 │  9.813487671828481 │\n│     2 │ 1.0368737381554407 │  9.942640301299889 │\n│     3 │ 1.0563527428724868 │  10.22310721615329 │\n│     4 │  1.043167490141166 │ 10.335489610548963 │\n│     5 │ 1.0780143169836582 │ 10.585156844133927 │\n│     6 │ 1.0910820093813216 │ 10.548651231531705 │\n│     7 │ 1.0593122394563554 │ 10.379943069577578 │\n│     8 │  1.079520899122755 │ 10.492650001889869 │\n│     9 │    1.2546008978996 │ 12.391197540031683 │\n│    10 │   1.28123927968838 │  12.50125248419417 │\n│    11 │ 1.2509031985271484 │  12.27013751494445 │\n│    12 │ 1.2376507362291627 │  12.31395285761324 │\n├───────┴────────────────────┴────────────────────┤\n│ 12 rows                               3 columns │\n└─────────────────────────────────────────────────┘\n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nRedo the above join but, rather than using CTEs, use tables that you first create in DuckDB’s memory bank. Again, this will simplify the actual join operation and also emphasise how crazy fast joins are in DuckDB.",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#windowing",
    "href": "duckdb-sql.html#windowing",
    "title": "DuckDB SQL",
    "section": "Windowing",
    "text": "Windowing\nOne last example: Binning “trip_distance” into deciles and then calculating the the mean “tip_amount” within each decile. This is an example of a window function and query pattern that I use all the time in my own work. I find it extremely useful for quickly pulling out descriptive patterns from large datasets, from which I can then develop a better intuition of my data. In turn, this shapes the hypotheses and modeling choices that I make in the subsequent analysis stage.\n\n\n\n\n\n\nSorting and sampling\n\n\n\nI’m using a 1% random sample of my data here (see the USING SAMPLE 1% statement). Why? Because calculating deciles requires ranking your data and this is expensive! To rank data, we first have to sort it (ORDER) and this requires evaluating/comparing every single row in your dataset. In turn, this means that it’s very hard to take shortcuts. (This is one reason why DuckDB’s optimized query plan will always delay sorting until as late as possible; to only sort on a smaller subset/aggregation of the data if possible.) FWIW, DuckDB’s sorting algorithm is still crazy fast. But for data of this size, and where sorting on the full datset is unavoidable, I strongly recommend sampling first. Your general insights will almost certainly remain intact.\n\n\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  WITH trip_deciles AS (\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      tip_amount,\n      trip_distance,\n      NTILE(10) OVER (ORDER BY trip_distance) AS decile\n    USING SAMPLE 1%\n  )\n  FROM trip_deciles\n  SELECT\n    decile,\n    AVG(trip_distance) AS mean_distance,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  \"\n)\n\n   decile mean_distance  mean_tip\n1       1     0.4447737 0.5654472\n2       2     0.8035106 0.5019584\n3       3     1.0565594 0.5995832\n4       4     1.3158020 0.6832499\n5       5     1.6161425 0.7753690\n6       6     1.9806189 0.8786339\n7       7     2.4748273 1.0192063\n8       8     3.2292295 1.2240061\n9       9     4.7245936 1.5924244\n10     10    11.0015967 3.2038042\n\n\n\n\n\ncon.query(\n  '''\n  WITH trip_deciles AS (\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      tip_amount,\n      trip_distance,\n      NTILE(10) OVER (ORDER BY trip_distance) AS decile\n    USING SAMPLE 1%\n  )\n  FROM trip_deciles\n  SELECT\n    decile,\n    AVG(trip_distance) AS mean_distance,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  '''\n)\n\n┌────────┬────────────────────┬────────────────────┐\n│ decile │   mean_distance    │      mean_tip      │\n│ int64  │       double       │       double       │\n├────────┼────────────────────┼────────────────────┤\n│      1 │ 0.4462739191673977 │ 0.5716254740492032 │\n│      2 │ 0.8055297973770881 │ 0.5000708635821985 │\n│      3 │ 1.0607604873791494 │ 0.5942867281760226 │\n│      4 │  1.321170068101389 │ 0.6758381506021942 │\n│      5 │ 1.6243709739488024 │ 0.7691185887885554 │\n│      6 │ 1.9912377356013096 │ 0.8836751743156347 │\n│      7 │ 2.4894762676127664 │ 1.0274908304845243 │\n│      8 │ 3.2452311476394033 │ 1.2313065408307358 │\n│      9 │  4.738938720005852 │ 1.5975793020874476 │\n│     10 │ 11.033802816138595 │ 3.2242121801505546 │\n├────────┴────────────────────┴────────────────────┤\n│ 10 rows                                3 columns │\n└──────────────────────────────────────────────────┘",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#close-connection",
    "href": "duckdb-sql.html#close-connection",
    "title": "DuckDB SQL",
    "section": "Close connection",
    "text": "Close connection\n\nRPython\n\n\n\ndbDisconnect(con)\n\nAgain, this step isn’t strictly necessary since we instantiated our connection with the shutdown = TRUE argument. But it’s worth seeing in case you want to be explicit.\n\n\n\ncon.close()",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#footnotes",
    "href": "duckdb-sql.html#footnotes",
    "title": "DuckDB SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAside: The shutdown = TRUE argument is a convenience feature that ensures our connection is automatically terminated when our R session ends (i.e., even if we forget to do it manually.) I’m not aware of a similar convenience argument for Python; please let me know if I am missing something.↩︎\nI’ll admit that the benefits don’t seem so great for this simple example. But trust me: they make a big difference once you start having lots of grouping columns and complex sub-queries.↩︎",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html",
    "href": "duckdb-dplyr.html",
    "title": "DuckDB + dplyr (R)",
    "section": "",
    "text": "library(duckdb)\n\nLoading required package: DBI\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#load-libraries",
    "href": "duckdb-dplyr.html#load-libraries",
    "title": "DuckDB + dplyr (R)",
    "section": "",
    "text": "library(duckdb)\n\nLoading required package: DBI\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#create-a-database-connection",
    "href": "duckdb-dplyr.html#create-a-database-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Create a database connection",
    "text": "Create a database connection\nFor the d(b)plyr workflow, the connection step is very similar to the pure SQL approach. The only difference is that, after instantiating the database connection, we need to register our parquet dataset as a table in our connection via the dplyr::tbl() function. Note that we also assign it to an object (here: nyc) that can be referenced from R.\n\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#first-example",
    "href": "duckdb-dplyr.html#first-example",
    "title": "DuckDB + dplyr (R)",
    "section": "First example",
    "text": "First example\nThis next command runs instantly because all computation is deferred (i.e., lazy eval). In other words, it is just a query object.\n\nq1 = nyc |&gt;\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n\n\n\n\n\n\n\n.by versus group_by\n\n\n\nIn case you weren’t aware: summarize(..., .by = x) is a shorthand (and non-persistent) version of group_by(x) |&gt; summarize(...). More details here.\n\n\nWe can see what DuckDB’s query tree looks like by asking it to explain the plan\n\nexplain(q1)\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n\n&lt;PLAN&gt;\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│    ────────────────────   │\n│         Groups: #0        │\n│    Aggregates: avg(#1)    │\n│                           │\n│       ~89814792 Rows      │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│    ────────────────────   │\n│      passenger_count      │\n│         tip_amount        │\n│                           │\n│      ~179629584 Rows      │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│    ────────────────────   │\n│         Function:         │\n│        READ_PARQUET       │\n│                           │\n│        Projections:       │\n│      passenger_count      │\n│         tip_amount        │\n│                           │\n│      ~179629584 Rows      │\n└───────────────────────────┘\n\n\nSimilarly, to show the SQL translation that will be implemented on the backend, using show_query.\n\nshow_query(q1)\n\n&lt;SQL&gt;\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n\n\nNote that printing the query object actually does enforce some computation. OTOH it’s still just a preview of the data (we haven’t pulled everything into R’s memory).\n\nq1\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.1.1 [root@Darwin 23.6.0:R 4.4.1/:memory:]\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               4    0.845\n 2               3    0.963\n 3               6    1.13 \n 4               8    0.351\n 5               7    0.544\n 6              66    1.5  \n 7               5    1.10 \n 8               9    0.807\n 9               0    0.862\n10             254    0    \n# ℹ more rows\n\n\nTo actually pull all of the result data into R, we must call collect() on the query object\n\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\n\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               0    0.862\n 2             249    0    \n 3             254    0    \n 4               2    1.08 \n 5             208    0    \n 6              10    0    \n 7               5    1.10 \n 8               9    0.807\n 9              65    0    \n10             177    1    \n11               7    0.544\n12               8    0.351\n13              66    1.5  \n14               3    0.963\n15               6    1.13 \n16               1    1.15 \n17             247    2.3  \n18               4    0.845\n\ntoc - tic\n\nTime difference of 2.044226 secs",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#aggregation",
    "href": "duckdb-dplyr.html#aggregation",
    "title": "DuckDB + dplyr (R)",
    "section": "Aggregation",
    "text": "Aggregation\nHere’s our earlier filtering example with multiple grouping + aggregation variables…\n\nq2 = nyc |&gt;\n  filter(month &lt;= 3) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n\n# Source:   SQL [?? x 4]\n# Database: DuckDB v1.1.1 [root@Darwin 23.6.0:R 4.4.1/:memory:]\n   month passenger_count tip_amount fare_amount\n   &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1     1               1      1.04         9.76\n 2     2               1      1.07         9.90\n 3     3               1      1.09        10.2 \n 4     1               3      0.875        9.87\n 5     1               6      1.02         9.86\n 6     2               3      0.895        9.98\n 7     2               6      1.02         9.96\n 8     3               6      1.05        10.3 \n 9     3               3      0.912       10.3 \n10     1               4      0.768       10.0 \n# ℹ more rows\n\n\nAside: note the optimised query includes hash groupings and projection (basically: fancy column subsetting, which is a suprisingly effective strategy in query optimization)\n\nexplain(q2)\n\n&lt;SQL&gt;\nSELECT\n  \"month\",\n  passenger_count,\n  AVG(tip_amount) AS tip_amount,\n  AVG(fare_amount) AS fare_amount\nFROM (\n  SELECT q01.*\n  FROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\n  WHERE (\"month\" &lt;= 3.0)\n) q01\nGROUP BY \"month\", passenger_count\n\n&lt;PLAN&gt;\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│    ────────────────────   │\n│          Groups:          │\n│             #0            │\n│             #1            │\n│                           │\n│        Aggregates:        │\n│          avg(#2)          │\n│          avg(#3)          │\n│                           │\n│       ~22453698 Rows      │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│    ────────────────────   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│        fare_amount        │\n│                           │\n│       ~44907396 Rows      │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│    ────────────────────   │\n│         Function:         │\n│        READ_PARQUET       │\n│                           │\n│        Projections:       │\n│      passenger_count      │\n│        fare_amount        │\n│         tip_amount        │\n│           month           │\n│                           │\n│       File Filters:       │\n│        (month &lt;= 3)       │\n│                           │\n│    Scanning Files: 3/12   │\n│                           │\n│       ~44907396 Rows      │\n└───────────────────────────┘\n\n\nAnd our high-dimensional aggregation example. We’ll create a query for this first, since I’ll reuse it shortly again\n\nq3 = nyc |&gt;\n  group_by(passenger_count, trip_distance) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)\n\n`summarise()` has grouped output by \"passenger_count\". You can override using\nthe `.groups` argument.\n\n\n# A tibble: 25,569 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance tip_amount fare_amount\n             &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1               1          17        5.64        47.0 \n 2               1           1.3      0.721        6.94\n 3               1           3        1.25        11.6 \n 4               1           7.2      2.27        21.3 \n 5               1           0.6      0.448        4.72\n 6               1           1.4      0.754        7.24\n 7               1           2        0.942        8.93\n 8               1           3.3      1.33        12.4 \n 9               2           2        0.811        8.99\n10               1           3.9      1.49        13.9 \n# ℹ 25,559 more rows",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#pivot-reshape",
    "href": "duckdb-dplyr.html#pivot-reshape",
    "title": "DuckDB + dplyr (R)",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\n\n# library(tidyr) ## already loaded\n\nq3 |&gt;\n  pivot_longer(tip_amount:fare_amount) |&gt;\n  collect()\n\n`summarise()` has grouped output by \"passenger_count\". You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by \"passenger_count\". You can override using\nthe `.groups` argument.\n\n\n# A tibble: 51,138 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance name       value\n             &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1               1          2.4  tip_amount 1.07 \n 2               1          3.1  tip_amount 1.28 \n 3               1         17.4  tip_amount 5.64 \n 4               1          1.24 tip_amount 0.671\n 5               1          4.9  tip_amount 1.73 \n 6               1          5.1  tip_amount 1.76 \n 7               0          4.5  tip_amount 1.40 \n 8               1          5.3  tip_amount 1.81 \n 9               0          2.7  tip_amount 1.01 \n10               2          5.1  tip_amount 1.63 \n# ℹ 51,128 more rows",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#joins-merges",
    "href": "duckdb-dplyr.html#joins-merges",
    "title": "DuckDB + dplyr (R)",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n\nmean_tips  = nyc |&gt; summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |&gt; summarise(mean_fares = mean(fare_amount), .by = month)\n\nAgain, these commands complete instantly because all computation has been deferred until absolutely necessary (i.e.,. lazy eval).\n\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |&gt;\n  collect()\n\nJoining with `by = join_by(month)`\n\n\n# A tibble: 12 × 3\n   month mean_fares mean_tips\n   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1     1       9.81      1.01\n 2    10      12.5       1.28\n 3     2       9.94      1.04\n 4    11      12.3       1.25\n 5     7      10.4       1.06\n 6     8      10.5       1.08\n 7    12      12.3       1.24\n 8     3      10.2       1.06\n 9     6      10.5       1.09\n10     4      10.3       1.04\n11     5      10.6       1.08\n12     9      12.4       1.25",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#windowing",
    "href": "duckdb-dplyr.html#windowing",
    "title": "DuckDB + dplyr (R)",
    "section": "Windowing",
    "text": "Windowing\nIf you recall from the native SQL API, we sampled 1 percent of the data before creating decile bins to reduce the computation burden of sorting the entire table. Unfortunately, this approach doesn’t work as well for the dplyr frontend because the underlying SQL translation uses a generic sampling approach (rather than DuckDB’s optimised USING SAMPLE statement.)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#close-connection",
    "href": "duckdb-dplyr.html#close-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Close connection",
    "text": "Close connection\n\ndbDisconnect(con)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#appendix-related-interfaces",
    "href": "duckdb-dplyr.html#appendix-related-interfaces",
    "title": "DuckDB + dplyr (R)",
    "section": "Appendix: Related interfaces",
    "text": "Appendix: Related interfaces\n\narrow+duckdb\n\nlibrary(arrow)\nlibrary(duckdb) ## Already loaded\nlibrary(dplyr)  ## Already loaded\nlibrary(tidyr)  ## Already loaded\n\nWhen going through the arrow intermediary, we don’t need to establish a database with DBI::dbConnect like we did above. Instead, we can create a link (pointers) to the dataset on disk directly via the arrow::open_dataset() convience function. Here I’ll assign it to a new R object called nyc2.\n\nnyc2 = open_dataset(\"nyc-taxi\")\n\n\n\n\n\n\n\nopen_dataset() versus read_parquet()\n\n\n\n(For individual parquet files, we could just read then via arrow::read_parquet(), perhaps efficiently subsetting columns at the same time. But I find the open_dataset is generally what I’m looking for.)\n\n\nNote that printing our nyc2 dataset to the R console will just display the data schema. This is a cheap and convenient way to quickly interrogate the basic structure of your data, including column types, etc.\n\nnyc2\n\nFileSystemDataset with 12 Parquet files\n24 columns\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\n...\n4 more columns\nUse `schema()` to see entire schema\n\n\nThe key step for this “arrow + duckdb” dplyr workflow is to pass our arrow dataset to DuckDB via the to_duckdb() function.\n\nto_duckdb(nyc2)\n\n# Source:   table&lt;arrow_001&gt; [?? x 24]\n# Database: DuckDB v1.1.1 [root@Darwin 23.6.0:R 4.4.1/:memory:]\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;                        &lt;dbl&gt;\n 1 CMT         2012-01-20 14:09:36 2012-01-20 14:42:25               1\n 2 CMT         2012-01-20 14:54:10 2012-01-20 15:06:55               1\n 3 CMT         2012-01-20 08:08:01 2012-01-20 08:11:02               1\n 4 CMT         2012-01-20 08:36:22 2012-01-20 08:39:44               1\n 5 CMT         2012-01-20 20:58:32 2012-01-20 21:03:04               1\n 6 CMT         2012-01-20 19:40:20 2012-01-20 19:43:43               2\n 7 CMT         2012-01-21 01:54:37 2012-01-21 02:08:02               2\n 8 CMT         2012-01-21 01:55:47 2012-01-21 02:08:51               3\n 9 VTS         2012-01-07 22:20:00 2012-01-07 22:27:00               2\n10 VTS         2012-01-10 07:11:00 2012-01-10 07:21:00               1\n# ℹ more rows\n# ℹ 20 more variables: trip_distance &lt;dbl&gt;, pickup_longitude &lt;dbl&gt;,\n#   pickup_latitude &lt;dbl&gt;, rate_code &lt;chr&gt;, store_and_fwd &lt;chr&gt;,\n#   dropoff_longitude &lt;dbl&gt;, dropoff_latitude &lt;dbl&gt;, payment_type &lt;chr&gt;,\n#   fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;, mta_tax &lt;dbl&gt;, tip_amount &lt;dbl&gt;,\n#   tolls_amount &lt;dbl&gt;, total_amount &lt;dbl&gt;, improvement_surcharge &lt;dbl&gt;,\n#   congestion_surcharge &lt;dbl&gt;, pickup_location_id &lt;dbl&gt;, …\n\n\nNote that this transfer from Arrow to DuckDB is very quick (and memory cheap) because it is a zero copy. We are just passing around pointers instead of actually moving any data. See this blog post for more details, but the high-level take away is that we are benefitting from the tightly integrated architectures of these two libraries.1\nAt this, point all of the regular dplyr workflow logic from above should carry over. Just remember to first pass the arrow dataset via the to_duckdb() funciton. For example, here’s our initial aggregation query again:\n\nnyc2 |&gt;\n  to_duckdb() |&gt; ## &lt;= key step\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |&gt;\n  collect()\n\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               5    1.10 \n 2               9    0.807\n 3              65    0    \n 4             177    1    \n 5               0    0.862\n 6             249    0    \n 7             254    0    \n 8               7    0.544\n 9               8    0.351\n10              66    1.5  \n11               3    0.963\n12               6    1.13 \n13               1    1.15 \n14             247    2.3  \n15               4    0.845\n16               2    1.08 \n17             208    0    \n18              10    0    \n\n\n\n\n\n\n\n\nArrow’s native acero engine\n\n\n\nSome of you may be used to performing computation with the arrow package without going through DuckDB. What’s happening here is that arrow provides its own computation engine called “acero”. This Arrow-native engine is actually pretty performant… albeit not a fast as DuckDB, nor as feature rich. So I personally recommend always passing to DuckDB if you can. Still, if you’re curious then you can test yourself by re-trying the code chunk, but commenting out the to_duckdb() line. For more details, see here.\n\n\n\n\nduckplyr\nThe new kid on the block is duckplyr (announcement / homepage). Without going into too much depth, the promise of duckplyr is that it can provide a “fully native” dplyr experience that is directly coupled to DuckDB’s query engine. So, for example, it won’t have to rely on DBI’s generic’ SQL translations. Instead, the relevant dplyr “verbs” are being directly translated to DuckDB’s relational API to construct logical query plans. If that’s too much jargon, just know that it should involve less overhead, fewer translation errors, and better optimization. Moreover, a goal of duckplyr is for it to be a drop-in replace for dplyr in general. In other words, you could just swap out library(dplyr) for library(duckplyr) and all of your data wrangling operations will come backed by the power of DuckDB. This includes for working on “regular” R data frames in memory.\nAll of this is exciting and I would urge you stay tuned. Right now, duckplyr is still marked as experimental and has a few rough edges. But the basics are there. For example:\n\nlibrary(duckplyr, warn.conflicts = FALSE)\n\nThe duckplyr package is configured to fall back to dplyr when it encounters an\nincompatibility. Fallback events can be collected and uploaded for analysis to\nguide future development. By default, no data will be collected or uploaded.\n→ Run `duckplyr::fallback_sitrep()` to review the current settings.\n\n\n✔ Overwriting dplyr methods with duckplyr methods.\nℹ Turn off with `duckplyr::methods_restore()`.\n\nduckplyr_df_from_parquet(\"nyc-taxi/**/*.parquet\") |&gt;\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n\nmaterializing:\n---------------------\n--- Relation Tree ---\n---------------------\nAggregate [passenger_count, mean(tip_amount)]\n  read_parquet(nyc-taxi/**/*.parquet)\n\n---------------------\n-- Result Columns  --\n---------------------\n- passenger_count (BIGINT)\n- mean_tip (DOUBLE)\n\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               2    1.08 \n 2             208    0    \n 3              10    0    \n 4               0    0.862\n 5             254    0    \n 6             249    0    \n 7               3    0.963\n 8               6    1.13 \n 9               4    0.845\n10               5    1.10 \n11               9    0.807\n12             177    1    \n13              65    0    \n14               1    1.15 \n15             247    2.3  \n16               8    0.351\n17               7    0.544\n18              66    1.5",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#footnotes",
    "href": "duckdb-dplyr.html#footnotes",
    "title": "DuckDB + dplyr (R)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Similar” might be a better description than “integrated”, since DuckdB does not use the Arrow memory model. But they are both columnar-orientated (among other things) and so the end result is pretty seamless integration.↩︎",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-julia.html",
    "href": "duckdb-julia.html",
    "title": "DuckDB in Julia",
    "section": "",
    "text": "We have a julia package as well for duckdb. 1",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#load-libraries",
    "href": "duckdb-julia.html#load-libraries",
    "title": "DuckDB in Julia",
    "section": "",
    "text": "We have a julia package as well for duckdb. 1",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#create-a-database-connection",
    "href": "duckdb-julia.html#create-a-database-connection",
    "title": "DuckDB in Julia",
    "section": "Create a database connection",
    "text": "Create a database connection\nWe proceed as before with the other languages:\n\nusing DuckDB\nusing DataFrames\nusing Dates\nusing Chain\ncon = DBInterface.connect(DuckDB.DB)\n\nDuckDB.DB(\":memory:\")\n\n\nLet’s run the first simple query again to get the number of rows, as before with R:\n\nnrowsdb = @chain con begin\n    DBInterface.execute(\n        \"\"\"\n        FROM 'nyc-taxi/**/*.parquet'\n        SELECT COUNT(passenger_count) as nrows\n        \"\"\"\n    )\nend\n\nncolsdb = DBInterface.execute(con,\n        \"\"\"\n        FROM 'nyc-taxi/**/*.parquet'\n         SELECT * LIMIT 1\n         \"\"\"\n         ) |&gt; DataFrame |&gt; names |&gt; length\n\n(rows = nrowsdb, cols = ncolsdb)\n\n(rows = (nrows = [178544324],), cols = 24)\n\n\n… and we know that this is a lot of data:\n\nrun(`du -h -d 0 nyc-taxi`)\n\n8.6G    nyc-taxi\n\n\nProcess(`du -h -d 0 nyc-taxi`, ProcessExited(0))",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#first-example",
    "href": "duckdb-julia.html#first-example",
    "title": "DuckDB in Julia",
    "section": "First example",
    "text": "First example\nSame examples as previously:\nSELECT\n  passenger_count,\n  AVG(tip_amount) AS mean_tip\nFROM 'nyc-taxi/**/*.parquet'\nGROUP BY passenger_count\nORDER BY passenger_count\n\ntime1 = @elapsed dat1 = DBInterface.execute(\n    con,\n    \"\"\"\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n        passenger_count,\n        AVG(tip_amount) AS mean_tip\n    GROUP BY ALL\n    ORDER BY ALL\n    \"\"\"\n    )\n\n6.27071575\n\n\nThat took 6.27 seconds. Here is the result:\n\nfirst(dat1,5)\n\n5-element Vector{Any}:\n Tables.ColumnsRow{Tables.CopiedColumns{@NamedTuple{passenger_count::Vector{Int64}, mean_tip::Vector{Float64}}}}: (passenger_count = 0, mean_tip = 0.8620988141425049)\n Tables.ColumnsRow{Tables.CopiedColumns{@NamedTuple{passenger_count::Vector{Int64}, mean_tip::Vector{Float64}}}}: (passenger_count = 1, mean_tip = 1.1510109615234965)\n Tables.ColumnsRow{Tables.CopiedColumns{@NamedTuple{passenger_count::Vector{Int64}, mean_tip::Vector{Float64}}}}: (passenger_count = 2, mean_tip = 1.081579842401206)\n Tables.ColumnsRow{Tables.CopiedColumns{@NamedTuple{passenger_count::Vector{Int64}, mean_tip::Vector{Float64}}}}: (passenger_count = 3, mean_tip = 0.9629493657893194)\n Tables.ColumnsRow{Tables.CopiedColumns{@NamedTuple{passenger_count::Vector{Int64}, mean_tip::Vector{Float64}}}}: (passenger_count = 4, mean_tip = 0.8445189789663262)",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#more-aggregation",
    "href": "duckdb-julia.html#more-aggregation",
    "title": "DuckDB in Julia",
    "section": "More Aggregation",
    "text": "More Aggregation\nLet’s try out some more aggregation queries. How about a slightly variation on a our first example query, where we (a) add “month” as a second grouping variable, and (b) subset to only the first three months of the year.\n\ntime2 = @elapsed dat1 = DBInterface.execute(\n    con,\n    \"\"\"\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n        passenger_count,\n        AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n    \"\"\"\n    )\n\n1.556352459\n\n\nThis time we clocked up 1.56 seconds - keep in mind that this is a lot of data to go through each time.",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#julia-specifics-register_data_frame",
    "href": "duckdb-julia.html#julia-specifics-register_data_frame",
    "title": "DuckDB in Julia",
    "section": "Julia Specifics: register_data_frame",
    "text": "Julia Specifics: register_data_frame\nOne cool thing about the julia package is that you can register a local dataframe into the database, which means that there is no copy oepration performed. Duckdb will directly read that julia dataframe instead:\n\nDuckDB.register_data_frame(con, dat1, \"dat1_on_db\")\n\n# let's read the first row back out just to check\nresults = DBInterface.execute(con, \n    \"\"\"\n    SELECT * FROM dat1_on_db\n    LIMIT 1\n    \"\"\"\n    )\nDataFrame(results)\n\n1×2 DataFrame\n\n\n\nRow\npassenger_count\nmean_tip\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1\n1.0655",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#julia-specifics-appender",
    "href": "duckdb-julia.html#julia-specifics-appender",
    "title": "DuckDB in Julia",
    "section": "Julia Specifics: Appender",
    "text": "Julia Specifics: Appender\nThe appender is an advanced duckdb feature which is optimized for fast row insertion into a database table. This feature is not available in neither R nor python packages. Here is an example. Let’s first create a new dataframe in your julia session and then read it row by row into an empty database table.\n\nndf = 100_000\ndf = DataFrame(\n        id        = collect(1:ndf),\n        value     = rand(Float32,ndf),\n        timestamp = Dates.now() + Dates.Second.(1:ndf),\n        date      = Dates.today() + Dates.Day.(1:ndf)\n    )\nfirst(df,5)\n\n5×4 DataFrame\n\n\n\nRow\nid\nvalue\ntimestamp\ndate\n\n\n\nInt64\nFloat32\nDateTime\nDate\n\n\n\n\n1\n1\n0.40374\n2024-10-29T22:43:07.086\n2024-10-30\n\n\n2\n2\n0.00279289\n2024-10-29T22:43:08.086\n2024-10-31\n\n\n3\n3\n0.346249\n2024-10-29T22:43:09.086\n2024-11-01\n\n\n4\n4\n0.950927\n2024-10-29T22:43:10.086\n2024-11-02\n\n\n5\n5\n0.875765\n2024-10-29T22:43:11.086\n2024-11-03\n\n\n\n\n\n\ncreate a new database with an empty table called test_table1:\n\ndb = DuckDB.DB()\n\nDBInterface.execute(db,\n    \"\"\"\n    CREATE OR REPLACE TABLE \n    test_table1(id INTEGER PRIMARY KEY, value FLOAT, timestamp TIMESTAMP, date DATE)\n    \"\"\")\n\n(Count = Int64[],)\n\n\nNow let us prepare a SQL statement, into which we will repeatedly insert the row for each data. This makes use of the positional parameters ?:\n\nstmt = DBInterface.prepare(db, \n    \"\"\"\n    INSERT INTO test_table1 VALUES(?,?,?,?)\n    \"\"\"\n    )\n\nDuckDB.Stmt(DuckDB.Connection(\":memory:\"), Ptr{Nothing} @0x00000001467995a0, \"INSERT INTO test_table1 VALUES(?,?,?,?)\\n\", DuckDB.MaterializedResult)\n\n\nWe are ready to time this now.\n\ne = @elapsed res = \n    begin\n        for r in eachrow(df)\n            DBInterface.execute(stmt, \n                (r.id, r.value, r.timestamp, r.date)\n            )\n        end\n        DBInterface.close!(stmt)  # close statement when done\n    end\n\n17.78447875\n\n\nOn my machine this took 17.78 seconds. This is probably not the most efficient way to get data into your database, but it could happen (if you create the data yourself from some modeling task, for example).\nNow, let’s try with the Appender API. We create an identical table, and attach an Appender object to it:\n\nDBInterface.execute(db,\n    \"CREATE OR REPLACE TABLE \n    test_table2(id INTEGER PRIMARY KEY, value FLOAT, timestamp TIMESTAMP, date DATE)\")\n\n# create an appender on the second table\nappender = DuckDB.Appender(db, \"test_table2\")\n\nDuckDB.Appender(Ptr{Nothing} @0x00000001170f33e0)\n\n\nlet’s see how fast that insert operation is now:\n\ne2 = @elapsed begin\n    for i in eachrow(df)\n        for j in i\n            DuckDB.append(appender, j)\n        end\n    DuckDB.end_row(appender)\n    end\n    DuckDB.close(appender)  # done now\nend\n\n0.227146958\n\n\nNow only 0.23 seconds only! Holy smokes.\n\n\n\n\n\n\nMeta Info\n\n\n\nFor the purpose of working with this notebook, we can currently only have a single quarto engine per notebook. While it’s perfectly possible to call both R and python code from a running julia session, one needs to annotate the code with some additional markup, which is not great for a learner. So I prefered to separate the julia version from the others. It’s most useful to create a local environment. To achieve this, you can open julia in the root of this document and do\n] activate .  # activate current dir in Pkg mode\nadd DuckDB\nadd DataFrames\nadd Dates\nadd Chain\nand quarto will pick up this env when it sees engine: julia in the frontmatter.",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "duckdb-julia.html#footnotes",
    "href": "duckdb-julia.html#footnotes",
    "title": "DuckDB in Julia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that there is currently a bug in the QuartoRunner setup. So, to run the quarto notebook, you have to use julia 1.10. For just executing the code, the latest julia version should work↩︎",
    "crumbs": [
      "DuckDB in Julia"
    ]
  },
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "Requirements",
    "section": "",
    "text": "In order to follow along with the examples in this workshop, you’ll need to install some R and/or Python packages, as well as download a reasonably large dataset. Please make sure that you have completed all requirements before the workshop starts!"
  },
  {
    "objectID": "requirements.html#r-and-python-packages",
    "href": "requirements.html#r-and-python-packages",
    "title": "Requirements",
    "section": "R and Python Packages",
    "text": "R and Python Packages\nFor this workshop, you have the option of following along in either R, Python, or both. Ideally, I’d recommend both since one of my goals is to demonstrate the close equivalency in workflows across languages. But I’ll leave that to you.1 In a separate notebook we added the Julia version of the main workshop content. While it’s perfectly possible to call both R and python code from a running julia session, one needs to annotate the code with some additional markup, which is not great for a learner.\n\nRPython (base)Python (conda)Julia\n\n\nRun the following commands in your R console.\ninstall.packages(c(\"duckdb\", \"arrow\", \"dplyr\", \"tidyr\", \"duckplyr\"))\npolars (and therefore tidypolars) are not on CRAN so we install them from R-universe. Details here.\nSys.setenv(NOT_CRAN = \"true\")\ninstall.packages(c(\"polars\", \"tidypolars\"), repos = \"https://community.r-multiverse.org\")\nNote that you will need polars &gt;= 0.19.1 and tidypolars &gt;= 0.10.1.\n\n\n\n\n\n\nR package binaries for Linux\n\n\n\nAre you an R user on a Linux machine? If so, I strongly recommend that you configure your user profile to pull in pre-compiled R package binaries for your distro from PPM, rather than installing source packages from CRAN (and then having to compile them on your own machine). This will greatly reduce installation times and other potential install headaches. If you haven’t done this already, or don’t know what I’m talking, then the simplest thing to do is to let the excellent rspm package (link) figure it out for you. Bonus: It will also resolve system dependencies at the same time.\n# Run these two commands before installing any other packages\ninstall.packages(\"rspm\")\nrspm::enable()\nP.S. Once you have installed the rspm package, you can add the following line to your ~/.Rprofile file and it will automatically figure everything out for you whenever you start a new R session. See the rspm website for additional tips around integration with renv projects and so on.\nsuppressMessages(rspm::enable())\n\n\n\n\nFirst create and activate a Python virtual environment from your terminal. (Important: I’ll assume that you are in the current root of this repo.) The exact command varies by operating system.\n# MacOS / Linux\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Windows\npy -m venv .venv\n.venv\\Scripts\\activate.bat\nThen install the Python packages that we will be using.\npython3 -m pip install duckdb polars pyarrow pandas matplotlib yaml -U\npython3 -m pip install 'ibis-framework[duckdb,polars]' -U\n\n\n\n\n\n\nVS Code\n\n\n\nIf you are using VS Code, then there are a few tweaks to this Python setup. First up, once you’ve create your .venv virtual environment, then should see a pop-up message to the effect of:\nWe noticed a new environment has been created. Do you want to select it for the\nworkspace folder?\nSelect “Yes”, then choose your Python interpreter (ideally Python 3.9 or higher).\nOnce that’s done, you will also need to install the ipykernel and jupyter packages in addition to the packages that I mentioned above. Moreover, I recommend install packages from within VS Code using cell magics, i.e.\n```{py}\n%pip install ipykernel jupyter -U\n%pip install duckdb polars pyarrow pandas -U\n%pip install 'ibis-framework[duckdb,polars]' -U\n```\n\n\n\n\nTo add a little to the confusion, you may also have a local conda installation to manage your python dependencies. If this is the case, you can create the relevant environment as follows:\nconda create --platform osx-arm64 -n duck-env duckdb pandas polars matplotlib ibis-framework pyarrow\nNotice that this call adjusts for the quirks of having a certain chipset on my Macbook Pro (M1), for which I needed to required the ARM64 platform version of python and packages. 🤷🏻‍♂️\n\n\nYou should get the latest version of julia, start it on your terminal at the root of this repository, and execute the following:\n] activate .  # hit ] and type `activate .`\nadd DuckDB\nadd DataFrames\nadd Dates\nadd Chain\nThis will download the required packages and add them to the local environement, which was created by the first line. This created a Project.toml file in the current directory (i.e. . in the activate above)."
  },
  {
    "objectID": "requirements.html#nyc-taxi-data",
    "href": "requirements.html#nyc-taxi-data",
    "title": "Requirements",
    "section": "NYC taxi data",
    "text": "NYC taxi data\nFor this workshop, we’ll make use of the infamous well-known New York City taxi data.\n\nWe’ll just be downloading a single year’s worth of data from 2012. But that will be enough to demonstrate the point and it’s of comparable size to the “typical” dataset that I work with.\nThe final dataset is ~8.5 GB compressed on disk and can take 10-20 minutes to download, depending on your internet connection.\n\nYou can download the dataset with the below terminal commands.\n\n\n\n\n\n\nNote\n\n\n\nYou will need the aws cli tool (install link) for these next commands to work. This should be a quick and simple install (you do not need a AWS account), but see further below for some alternative download options.\n\n\nmkdir -p nyc-taxi/year=2012\naws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012 nyc-taxi/year=2012 --recursive --no-sign-request\nBesides being relatively chonky (8.6Gb), there are two features of this dataset that we’ll come back to since they are key to our workflow:\n\nThe data are stored in .parquet file format.\nThese Parquet files are organised in so-called “Hive-style” partitions on disk.\n\n\nOther data options\n\nSmaller subsets of the data\nIf you’re pressed for time and/or disk space, feel free to only grab a subset of the data manually. But make sure that you preserve the Hive-style partitioning. Here’s a quick example of how to do it for the first two months.\nmkdir -p nyc-taxi/year=2012/month=1\nmkdir -p nyc-taxi/year=2012/month=2\naws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012/month=1/ nyc-taxi/year=2012/month=1 --recursive --no-sign-request\naws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012/month=2/ nyc-taxi/year=2012/month=2 --recursive --no-sign-request\n\n\nAlternative download options\nIf you don’t have the aws cli tool, or can’t install install it for some reason, then you can always download the dataset directly from R or Python using some of the packages that we installed above. For example:\nlibrary(arrow)\nlibrary(dplyr)\n\ndata_path = \"nyc-taxi/year=2012\" # Or set your own preferred path\n\nopen_dataset(\"s3://voltrondata-labs-datasets/nyc-taxi/year=2012\") |&gt;\n    write_dataset(data_path, partitioning = \"month\")\nBe forewarned that these alternative download approaches are going to be slower than the aws cli approach."
  },
  {
    "objectID": "requirements.html#footnotes",
    "href": "requirements.html#footnotes",
    "title": "Requirements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re unsure and just want to pick one, then I recommend R. It’s much easier to install and manage environments. Plus it’s also my preferred language, so you’re likely to get better support from me.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "",
    "text": "This is a clone of Grant McDermott’s wonderful workshop hosted here. What you see here is a copy of his work, to which I (Florian Oswald added the julia version of duckdb. I also added the relevant julia setup in requirements. Grant has not vetted those additions in any way, so any additional errors were introduced without his knowledge. Please checkout the source code for his original work, and make a fork from it, if you want to use it, on github. Thanks Grant as ever for sharing those great resources with the rest of the world!"
  },
  {
    "objectID": "index.html#disclaimer-this-is-a-clone-made-by-florian-oswald",
    "href": "index.html#disclaimer-this-is-a-clone-made-by-florian-oswald",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "",
    "text": "This is a clone of Grant McDermott’s wonderful workshop hosted here. What you see here is a copy of his work, to which I (Florian Oswald added the julia version of duckdb. I also added the relevant julia setup in requirements. Grant has not vetted those additions in any way, so any additional errors were introduced without his knowledge. Please checkout the source code for his original work, and make a fork from it, if you want to use it, on github. Thanks Grant as ever for sharing those great resources with the rest of the world!"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Description",
    "text": "Description\nThis workshop will introduce you to DuckDB and Polars, two data wrangling libraries at the frontier of high-performance computation. (See benchmarks.) In addition to being extremely fast and portable, both DuckDB and Polars provide user-friendly implementations across multiple languages. This makes them very well suited to production and applied research settings, without the overhead of tools like Spark. We will provide a variety of real-life examples in both R and Python, with the aim of getting participants up and running as quickly as possible. We will learn how wrangle datasets extending over several hundred million observations in a matter of seconds or less, using only our laptops. And we will learn how to scale to even larger contexts where the data exceeds our computers’ RAM capacity. Finally, we will also discuss some complementary tools and how these can be integrated for an efficient end-to-end workflow (data I/O -&gt; wrangling -&gt; analysis).\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe content for this workshop has been prepared, and is presented, in my personal capacity. Any opinions expressed herein are my own and are not necessarily shared by my employer. Please do not share any recorded material without the express permission of myself or the workshop organisers."
  },
  {
    "objectID": "slides/slides.html#preliminaries",
    "href": "slides/slides.html#preliminaries",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Preliminaries",
    "text": "Preliminaries\nAgenda and expectations\nThese sparse slides are mostly intended to serve as a rough guide map.\n\nMost of what we’ll be doing is live coding and working through examples.\nI strongly encourage you try these examples on you own machines. Laptops are perfectly fine.\n\nNote: All of the material for today’s workshop are available on my website:\n\nhttps://grantmcdermott.com/duckdb-polars",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#preliminaries-1",
    "href": "slides/slides.html#preliminaries-1",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Preliminaries",
    "text": "Preliminaries\nRequirements\nImportant: Before continuing, please make sure that you have completed the requirements listed on the workshop website.\n\nInstall the required R and/or Python libraries.\nDownload some NYC taxi data.\n\nThe data download step can take 15-20 minutes, depending on your internet connection.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#problem-statement",
    "href": "slides/slides.html#problem-statement",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Problem statement",
    "text": "Problem statement\nWhy this workshop?\nIt’s a trope, but “big data” is everywhere. This is true whether you work in tech (like I do now), or in academic research (like I used to).\nOTOH many of datasets that I find myself working with aren’t at the scale of truly huge data that might warrant a Spark cluster.\n\nWe’re talking anywhere between 100 MB to 50 GB. (Max a few billion rows; often in the millions or less.)\nCan I do my work without the pain of going through Spark?\n\nAnother factor is working in polyglot teams. It would be great to repurpose similar syntax and libraries across languages…",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#taster",
    "href": "slides/slides.html#taster",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Taster",
    "text": "Taster\nDuckDB example\n\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\nnyc = open_dataset(here::here(\"nyc-taxi\"))\nprettyNum(nrow(nyc), \",\")\n\n[1] \"178,544,324\"\n\ntic = Sys.time()\n\nnyc_summ = nyc |&gt;\n  to_duckdb() |&gt;\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |&gt; \n  collect()\n\n(toc = Sys.time() - tic)\n\nTime difference of 1.606904 secs",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#taster-1",
    "href": "slides/slides.html#taster-1",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Taster",
    "text": "Taster\nDuckDB example (cont.)\nWe just read a ~180 million row dataset (from disk!) and did a group-by aggregation on it.\nIn &lt; 1 second.\nOn a laptop.\n🤯\n\n Let’s do a quick horesrace comparison (similar grouped aggregation, but on a slightly smaller dataset)…",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#simple-benchmark-computation-time-only",
    "href": "slides/slides.html#simple-benchmark-computation-time-only",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Simple benchmark: Computation time only",
    "text": "Simple benchmark: Computation time only\nDuckDB and Polars are already plenty fast…",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#simple-benchmark-computation-time-data-io",
    "href": "slides/slides.html#simple-benchmark-computation-time-data-io",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Simple benchmark: Computation time + data I/O",
    "text": "Simple benchmark: Computation time + data I/O\n… but are even more impressive once we account for data import times",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#wait.-how",
    "href": "slides/slides.html#wait.-how",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Wait. How??",
    "text": "Wait. How??\nBetter disk storage 🤝 Better memory representation\nTwo coinciding (r)evolutions enable faster, smarter computation:\n\n\n1. Better on-disk storage\n\nMainly talking about the Parquet file format here.\nColumnar storage format allows better compression (much smaller footprint) and efficient random access to selected rows or columns (don’t have to read the whole dataset a la CSVs).\n\n\n2. Better in-memory representation\n\nStandardisation around the Apache Arrow format + columnar representation. (Allows zero copy, fewer cache misses, etc.)\nOLAP + deferred materialisation. (Rather than “eagerly” executing each query step, we can be “lazy” and optimise queries before executing them.)",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#scaling-up",
    "href": "slides/slides.html#scaling-up",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Scaling up",
    "text": "Scaling up\nEven moar benchmarks\nQuestion: Do these benchmarks hold and scale more generally? Answer: Yes. See Database-like ops benchmark.\nMoreover—and I think this is key—these kinds of benchmarks normally exclude the data I/O component… and the associated benefits of not having to hold the whole dataset in RAM.\n\nThere are some fantastically fast in-memory data wrangling libraries out there. (My personal faves: data.table and collapse.) But “in-memory” means that you always have to keep the full dataset in, well, memory. And this can be expensive.\nLibraries like DuckDB and Polars sidestep this problem, effectively supercharging your computer’s data wrangling powers.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#examples",
    "href": "slides/slides.html#examples",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Examples",
    "text": "Examples\nLive coding sessions\nLet’s head back to the website to work through some notebooks.\nDuckDB\n\nDuckDB SQL\nDuckDB + dplyr (R)\nDuckDB + Ibis (Python)\n\nPolars\n\nPolars from R and Python",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#what-didnt-we-cover",
    "href": "slides/slides.html#what-didnt-we-cover",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\nOther cool features\n\nS3 I/O\n\nDuckDB & Polars can both read/write directly from/to S3. You just need to provision your AWS credentials. [Ex. 1, 2, 3]\nNote: I prefer/recommend the workflow we practiced today—first download to local disk via aws cli—to avoid network + I/O latency.\n\nGeospatial\n\nIMO the next iteration of geospatial computation will be built on top of the tools we’ve seen today (and related libs).\nDuckDB provides an excellent spatial extension (works with dplyr). See also the GeoParquet, GeoArrow, & GeoPolars initiatives.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#what-didnt-we-cover-1",
    "href": "slides/slides.html#what-didnt-we-cover-1",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\nOther cool features (cont.)\n\nStreaming\n\nStreaming is the feature that enables working with bigger-than-RAM data.\nVery easy to use and/or adjust our workflow to these cases…\nDuckDB: Simply specify a disk-backed database when you first fire up your connection from Python or R, e.g.\n\ncon = dbConnect(duckdb(), dbdir = \"nyc.dbb\")\n\nPolars: Simply specify streaming when collecting, e.g.\n\nsome_query.collect(streaming=True)",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#what-didnt-we-cover-2",
    "href": "slides/slides.html#what-didnt-we-cover-2",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\nOther cool features (cont.)\n\nModeling\n\nThe modeling part of this workflow is less tightly integrated b/c we generally have to bring the data into RAM.\nBut being able to quickly I/O parts of large datasets makes it very easy to iteratively run analyses on subsets of your data. E.g., I typically pair with fixest for unmatched performance on high-dimensional data.\nYou can also run bespoke models via UDFs and/or predictions on database backends. [Ex. 1, 2, 3]\nFWIW I believe that the underlying matrix and linear algebra libraries for direct modeling with these tools are coming. [Ex. 1, 2]",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#resources",
    "href": "slides/slides.html#resources",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Resources",
    "text": "Resources\nLearning more\nDuckDB\n\nDuckDB homepage. Includes a very informative blog and standalone documentation for the client APIs (Python, R, and many others).\nAlso check out Harlequin for a cool, shell-based DuckDB IDE.\n\nPolars\n\nPolars GitHub Repo. Contains links to the standalone documentation for the client APIS (Python, R, etc.)\nSide-by-side code comparisons (versus pandas, dplyr, etc.) are available in Modern Polars (in Python) and Codebook for Polars in R.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "duckdb-ibis.html",
    "href": "duckdb-ibis.html",
    "title": "DuckDB + Ibis (Python)",
    "section": "",
    "text": "import ibis\nimport ibis.selectors as s\nfrom ibis import _\n# ibis.options.interactive = True # enforce eager execution of queries",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#load-libraries",
    "href": "duckdb-ibis.html#load-libraries",
    "title": "DuckDB + Ibis (Python)",
    "section": "",
    "text": "import ibis\nimport ibis.selectors as s\nfrom ibis import _\n# ibis.options.interactive = True # enforce eager execution of queries",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#connect-and-register",
    "href": "duckdb-ibis.html#connect-and-register",
    "title": "DuckDB + Ibis (Python)",
    "section": "Connect and register",
    "text": "Connect and register\n\n## Instantiate an in-memory DuckDB connection from Ibis\ncon = ibis.duckdb.connect()\n\n## Register our parquet dataset as a table called \"nyc\" in our connection\ncon.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n\n/var/folders/5q/yhcyv3z55wvg6lhgc3h22kk00000gq/T/ipykernel_39424/2359419501.py:5: FutureWarning: `Backend.register` is deprecated as of v9.1; use the explicit `read_*` method for the filetype you are trying to read, e.g., read_parquet, read_csv, etc.\n  con.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n\n\nDatabaseTable: nyc\n  vendor_name           string\n  pickup_datetime       timestamp(6)\n  dropoff_datetime      timestamp(6)\n  passenger_count       int64\n  trip_distance         float64\n  pickup_longitude      float64\n  pickup_latitude       float64\n  rate_code             string\n  store_and_fwd         string\n  dropoff_longitude     float64\n  dropoff_latitude      float64\n  payment_type          string\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  total_amount          float64\n  improvement_surcharge float64\n  congestion_surcharge  float64\n  pickup_location_id    int64\n  dropoff_location_id   int64\n  month                 int64\n  year                  int64\n\n\n\nAside: Remember that you can create a persistent, disk-backed database by giving it an appropriate name/path. This also enables out-of-core computation for bigger than RAM data.\n# con = ibis.duckdb.connect(\"nyc.dbb\")\n# con.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n# etc.\nReference the table from Python. We’ll call this reference object nyc too for consistency, but you could call it whatever you want (e.g., you could call it nyc_ibis to avoid potential ambiguity with the “nyc” table in our actual DuckDB connection). Printing the object to screen will give us a lazy preview.\n\n# con.list_tables() # Optional: confirm that our table is available\n\nnyc = con.table(\"nyc\")\nnyc\n\nDatabaseTable: nyc\n  vendor_name           string\n  pickup_datetime       timestamp(6)\n  dropoff_datetime      timestamp(6)\n  passenger_count       int64\n  trip_distance         float64\n  pickup_longitude      float64\n  pickup_latitude       float64\n  rate_code             string\n  store_and_fwd         string\n  dropoff_longitude     float64\n  dropoff_latitude      float64\n  payment_type          string\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  total_amount          float64\n  improvement_surcharge float64\n  congestion_surcharge  float64\n  pickup_location_id    int64\n  dropoff_location_id   int64\n  month                 int64\n  year                  int64",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#first-example",
    "href": "duckdb-ibis.html#first-example",
    "title": "DuckDB + Ibis (Python)",
    "section": "First example",
    "text": "First example\n\nq1 = (\n  nyc\n  .group_by([\"passenger_count\"])\n  .agg(mean_tip = _.tip_amount.mean())\n)\n\nTo see the underlying SQL translation, use ibis.to_sql()\n\nibis.to_sql(q1)\n\nSELECT\n  \"t0\".\"passenger_count\",\n  AVG(\"t0\".\"tip_amount\") AS \"mean_tip\"\nFROM \"nyc\" AS \"t0\"\nGROUP BY\n  1\n\n\nTo actually execute the query and bring the result into Python, we can use the execute() method. By default this will coerce to a pandas DataFrame.\n\ndat1 = q1.execute()\ndat1\n\n\n\n\n\n\n\n\n\n\n\npassenger_count\nmean_tip\n\n\n\n\n0\n0\n0.862099\n\n\n1\n249\n0.000000\n\n\n2\n254\n0.000000\n\n\n3\n5\n1.102732\n\n\n4\n9\n0.806800\n\n\n5\n65\n0.000000\n\n\n6\n177\n1.000000\n\n\n7\n2\n1.081580\n\n\n8\n208\n0.000000\n\n\n9\n10\n0.000000\n\n\n10\n3\n0.962949\n\n\n11\n6\n1.128365\n\n\n12\n1\n1.151011\n\n\n13\n247\n2.300000\n\n\n14\n4\n0.844519\n\n\n15\n7\n0.544118\n\n\n16\n8\n0.350769\n\n\n17\n66\n1.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIbis conversion to polars\n\n\n\nThe q1.execute() method above is equivalent calling q1.to_pandas(). A q1.to_polars() equivalent has been added to the dev version of Ibis, but is not available with the latest offical release (8.0.0 at the time of writing).\n\n\n\nDigression: Interactive Ibis use and eager execution\nAt the very top of this document, I commented out the ibis.options.interactive option as part of my Ibis configuration. This was because I wanted to demonstrate the default deferred (i.e., lazy) behaviour of Ibis, which is just the same as d(b)plyr in R. If you are building data wrangling pipelines, or writing scripts with potentially complex queries, you probably want to preserve this deferred behaviour and avoid eager execution.\nHowever, there are times when you may want to default into eager execution. For example, if your dataset is of manageable size, or you are trying to iterate through different query operations… Or, you might just want to enable it so that you automatically get a nice print return object for your workshop materials. I’ll adopt the latter view, so that I can quickly demonstrate some Ibis syntax and results for the rest of this document.\n\nibis.options.interactive = True\n\nOkay, let’s speed through some of the same basic queries that we’ve already seen in the DuckDB SQL and R (dplyr) pages. I won’t bother to explain them in depth. Just consider them for demonstration purposes.",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#aggregation",
    "href": "duckdb-ibis.html#aggregation",
    "title": "DuckDB + Ibis (Python)",
    "section": "Aggregation",
    "text": "Aggregation\n\n(\n  nyc\n  .group_by([\"passenger_count\", \"trip_distance\"])\n  .aggregate(\n    mean_tip = _.tip_amount.mean(),\n    mean_fare = _.fare_amount.mean()\n    )\n)\n\n\n\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ passenger_count ┃ trip_distance ┃ mean_tip ┃ mean_fare ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64           │ float64       │ float64  │ float64   │\n├─────────────────┼───────────────┼──────────┼───────────┤\n│               2 │          1.80 │ 0.753793 │  8.450206 │\n│               1 │          0.20 │ 0.386448 │  4.671193 │\n│               1 │          3.47 │ 1.307350 │ 12.758321 │\n│               1 │          4.70 │ 1.679448 │ 15.728522 │\n│               1 │          2.70 │ 1.162573 │ 10.811488 │\n│               1 │          1.80 │ 0.878870 │  8.402046 │\n│               2 │          3.80 │ 1.324273 │ 13.787123 │\n│               3 │          1.28 │ 0.616118 │  6.737016 │\n│               1 │         14.20 │ 4.238410 │ 38.186588 │\n│               2 │          2.20 │ 0.871988 │  9.536184 │\n│               … │             … │        … │         … │\n└─────────────────┴───────────────┴──────────┴───────────┘\n\n\n\nNote that, even though we have enabled the interactive print mode, we still get lazy evalation if we assign a chain of query steps to an object (here: q3)…\n\nq3 = (\n  nyc\n  .group_by([\"passenger_count\", \"trip_distance\"])\n  .agg(\n    mean_tip = _.tip_amount.mean(),\n    mean_fare = _.fare_amount.mean()\n    )\n)\n\n… but printing the query to screen enforces computation.\n\nq3\n\n\n\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ passenger_count ┃ trip_distance ┃ mean_tip ┃ mean_fare ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64           │ float64       │ float64  │ float64   │\n├─────────────────┼───────────────┼──────────┼───────────┤\n│               1 │           3.3 │ 1.326239 │ 12.390759 │\n│               1 │           0.6 │ 0.447695 │  4.717754 │\n│               1 │           1.1 │ 0.653059 │  6.333355 │\n│               3 │           3.9 │ 1.150150 │ 14.132481 │\n│               1 │           1.2 │ 0.688208 │  6.640748 │\n│               1 │           1.3 │ 0.720728 │  6.941542 │\n│               1 │           3.9 │ 1.488901 │ 13.855821 │\n│               1 │           3.0 │ 1.250686 │ 11.611495 │\n│               1 │          21.0 │ 5.529616 │ 48.682804 │\n│               1 │           2.0 │ 0.942336 │  8.932499 │\n│               … │             … │        … │         … │\n└─────────────────┴───────────────┴──────────┴───────────┘",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#pivot-reshape",
    "href": "duckdb-ibis.html#pivot-reshape",
    "title": "DuckDB + Ibis (Python)",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\n\n# now chain on pivoting (and enforce computation via printing)\n(\n  q3\n  .pivot_longer(s.r[\"mean_tip\":\"mean_fare\"])\n)\n\n/Users/floswald/git/grant/duckdb-polars/.venv/lib/python3.12/site-packages/ibis/selectors.py:82: FutureWarning: `r` is deprecated as of v9.5; use `ibis.selectors.index` instead\n  util.warn_deprecated(\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ passenger_count ┃ trip_distance ┃ name      ┃ value     ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64           │ float64       │ string    │ float64   │\n├─────────────────┼───────────────┼───────────┼───────────┤\n│               1 │           1.0 │ mean_tip  │  0.615529 │\n│               1 │           1.0 │ mean_fare │  6.012116 │\n│               3 │           0.8 │ mean_tip  │  0.414165 │\n│               3 │           0.8 │ mean_fare │  5.470059 │\n│               1 │           0.8 │ mean_tip  │  0.534714 │\n│               1 │           0.8 │ mean_fare │  5.360443 │\n│               3 │           7.3 │ mean_tip  │  1.850546 │\n│               3 │           7.3 │ mean_fare │ 22.016939 │\n│               1 │           0.4 │ mean_tip  │  0.361255 │\n│               1 │           0.4 │ mean_fare │  4.173765 │\n│               … │             … │ …         │         … │\n└─────────────────┴───────────────┴───────────┴───────────┘",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#joins-merges",
    "href": "duckdb-ibis.html#joins-merges",
    "title": "DuckDB + Ibis (Python)",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n(As we did in the dplyr code, we’ll break this contrived join example into two steps)\n\nmean_tips = nyc.group_by(\"month\").agg(mean_tip = _.tip_amount.mean())\nmean_fares = nyc.group_by(\"month\").agg(mean_fare = _.fare_amount.mean())\n\n\n(\n  mean_tips\n  .left_join(mean_fares, \"month\")\n)\n\n\n\n\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ month ┃ mean_tip ┃ month_right ┃ mean_fare ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64 │ float64  │ int64       │ float64   │\n├───────┼──────────┼─────────────┼───────────┤\n│     4 │ 1.043167 │           4 │ 10.335490 │\n│     1 │ 1.007817 │           1 │  9.813488 │\n│    10 │ 1.281239 │          10 │ 12.501252 │\n│     2 │ 1.036874 │           2 │  9.942640 │\n│    11 │ 1.250903 │          11 │ 12.270138 │\n│     5 │ 1.078014 │           5 │ 10.585157 │\n│     9 │ 1.254601 │           9 │ 12.391198 │\n│     7 │ 1.059312 │           7 │ 10.379943 │\n│     8 │ 1.079521 │           8 │ 10.492650 │\n│    12 │ 1.237651 │          12 │ 12.313953 │\n│     … │        … │           … │         … │\n└───────┴──────────┴─────────────┴───────────┘",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "polars-rpy.html",
    "href": "polars-rpy.html",
    "title": "Polars from Python and R",
    "section": "",
    "text": "PythonR\n\n\n\nimport polars as pl\nimport time\nimport matplotlib\n\n\n\n\nlibrary(polars)",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#load-libraries",
    "href": "polars-rpy.html#load-libraries",
    "title": "Polars from Python and R",
    "section": "",
    "text": "PythonR\n\n\n\nimport polars as pl\nimport time\nimport matplotlib\n\n\n\n\nlibrary(polars)",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#scan-data",
    "href": "polars-rpy.html#scan-data",
    "title": "Polars from Python and R",
    "section": "Scan data",
    "text": "Scan data\n\nPythonR\n\n\n\nnyc = pl.scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning=True)\nnyc\n\nnaive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n    \n    Parquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other sources]PROJECT */24 COLUMNS\n\n\n\n\n\nnyc = pl$scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning=TRUE)\nnyc\n\npolars LazyFrame\n $explain(): Show the optimized query plan.\n\nNaive plan:\nParquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other sources]\nPROJECT */24 COLUMNS",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#first-example",
    "href": "polars-rpy.html#first-example",
    "title": "Polars from Python and R",
    "section": "First example",
    "text": "First example\nPolars operations are registered as queries until they are collected.\n\nPythonR\n\n\n\nq1 = (\n    nyc\n    .group_by([\"passenger_count\"])\n    .agg([\n            pl.mean(\"tip_amount\")#.alias(\"mean_tip\") ## alias is optional\n        ])\n    .sort(\"passenger_count\")\n)\nq1\n\nnaive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n    \n    SORT BY [col(\"passenger_count\")]  AGGREGATE      [col(\"tip_amount\").mean()] BY [col(\"passenger_count\")] FROM    Parquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other sources]    PROJECT */24 COLUMNS\n\n\n\n\n\nq1 = (\n    nyc\n    $group_by(\"passenger_count\")\n    $agg(\n        pl$mean(\"tip_amount\")#$alias(\"mean_tip\") ## alias is optional\n    )\n    $sort(\"passenger_count\")\n)\nq1 \n\npolars LazyFrame\n $explain(): Show the optimized query plan.\n\nNaive plan:\nSORT BY [col(\"passenger_count\")]\n  AGGREGATE\n    [col(\"tip_amount\").mean()] BY [col(\"passenger_count\")] FROM\n    Parquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other sources]\n    PROJECT */24 COLUMNS\n\n\n\n\n\n\n\n\nR-polars multiline syntax\n\n\n\nPolars-style x$method1()$method2()... chaining may seem a little odd to R users, especially for multiline queries. Here I have adopted the same general styling as Python: By enclosing the full query in parentheses (), we can start each $method() on a new line. If this isn’t to your fancy, you could also rewrite these multiline queries as follows:\nnyc$group_by(\n    \"passenger_count\"\n)$agg(\n    pl$mean(\"tip_amount\")\n)$sort(\"passenger_count\")\n\n\n\n\n\n(Note: this is the naive query plan, not the optimized query that polars will actually implement for us. We’ll come back to this idea shortly.)\nCalling collect() enforces computation.\n\nPythonR\n\n\n\ntic = time.time()\ndat1 = q1.collect()\ntoc = time.time()\n\ndat1\n\n\nshape: (18, 2)\n\n\n\npassenger_count\ntip_amount\n\n\ni64\nf64\n\n\n\n\n0\n0.862099\n\n\n1\n1.151011\n\n\n2\n1.08158\n\n\n3\n0.962949\n\n\n4\n0.844519\n\n\n…\n…\n\n\n177\n1.0\n\n\n208\n0.0\n\n\n247\n2.3\n\n\n249\n0.0\n\n\n254\n0.0\n\n\n\n\n\n# print(f\"Time difference of {toc - tic} seconds\")\n\n\n\n\ntic = Sys.time()\ndat1 = q1$collect()\ntoc = Sys.time()\n\ndat1\n\n\nshape: (18, 2)\n\n\n\npassenger_count\ntip_amount\n\n\ni64\nf64\n\n\n\n\n0\n0.862099\n\n\n1\n1.151011\n\n\n2\n1.08158\n\n\n3\n0.962949\n\n\n4\n0.844519\n\n\n…\n…\n\n\n177\n1.0\n\n\n208\n0.0\n\n\n247\n2.3\n\n\n249\n0.0\n\n\n254\n0.0\n\n\n\n\n\ntoc - tic\n\nTime difference of 1.1572 secs",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#aggregation",
    "href": "polars-rpy.html#aggregation",
    "title": "Polars from Python and R",
    "section": "Aggregation",
    "text": "Aggregation\nSubsetting along partition dimensions allows for even more efficiency gains.\n\nPythonR\n\n\n\nq2 = (\n    nyc\n    .filter(pl.col(\"month\") &lt;= 3)\n    .group_by([\"month\", \"passenger_count\"])\n    .agg([pl.mean(\"tip_amount\").alias(\"mean_tip\")])\n    .sort(\"passenger_count\")\n)\n\n\n\n\nq2 = (\n    nyc\n    $filter(pl$col(\"month\") &lt;= 3)\n    $group_by(\"month\", \"passenger_count\")\n    $agg(pl$mean(\"tip_amount\")$alias(\"mean_tip\"))\n    $sort(\"passenger_count\")\n) \n\n\n\n\nLet’s take a look at the optimized query that Polars will implement for us.\n\nPythonR\n\n\n\n# q2             # naive\n# q2.show_graph()  # optimized\n\n\n\n\n# q2              # naive\ncat(q2$explain()) # optimized\n\nSORT BY [col(\"passenger_count\")]\n  AGGREGATE\n    [col(\"tip_amount\").mean().alias(\"mean_tip\")] BY [col(\"month\"), col(\"passenger_count\")] FROM\n    Parquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other sources]\n    PROJECT 2/24 COLUMNS\n    SELECTION: [(col(\"month\").cast(Unknown(Float))) &lt;= (dyn float: 3.0)]\n\n\n\n\n\nNow, let’s run the query and collect the results.\n\nPythonR\n\n\n\ntic = time.time()\ndat2 = q2.collect()\ntoc = time.time()\n\ndat2\n\n\nshape: (29, 3)\n\n\n\nmonth\npassenger_count\nmean_tip\n\n\ni64\ni64\nf64\n\n\n\n\n3\n0\n0.877675\n\n\n1\n0\n0.841718\n\n\n2\n0\n0.876637\n\n\n1\n1\n1.036863\n\n\n3\n1\n1.089205\n\n\n…\n…\n…\n\n\n2\n9\n0.0\n\n\n1\n9\n0.0\n\n\n1\n65\n0.0\n\n\n1\n208\n0.0\n\n\n3\n208\n0.0\n\n\n\n\n\n# print(f\"Time difference of {toc - tic} seconds\")\n\n\n\n\ntic = Sys.time()\ndat2 = q2$collect()\ntoc = Sys.time()\n\ndat2\n\n\nshape: (29, 3)\n\n\n\nmonth\npassenger_count\nmean_tip\n\n\ni64\ni64\nf64\n\n\n\n\n2\n0\n0.876637\n\n\n1\n0\n0.841718\n\n\n3\n0\n0.877675\n\n\n2\n1\n1.06849\n\n\n1\n1\n1.036863\n\n\n…\n…\n…\n\n\n2\n9\n0.0\n\n\n1\n9\n0.0\n\n\n1\n65\n0.0\n\n\n3\n208\n0.0\n\n\n1\n208\n0.0\n\n\n\n\n\ntoc - tic\n\nTime difference of 1.016284 secs\n\n\n\n\n\nHigh-dimensional grouping example. This query provides an example where polars is noticeably slower than DuckDB.\n\nPythonR\n\n\n\nq3 = (\n    nyc\n    .group_by([\"passenger_count\", \"trip_distance\"])\n    .agg([\n        pl.mean(\"tip_amount\").alias(\"mean_tip\"),\n        pl.mean(\"fare_amount\").alias(\"mean_fare\"),\n        ])\n    .sort([\"passenger_count\", \"trip_distance\"])\n)\n\ntic = time.time()\ndat3 = q3.collect()\ntoc = time.time()\n\ndat3\n\n\nshape: (25_569, 4)\n\n\n\npassenger_count\ntrip_distance\nmean_tip\nmean_fare\n\n\ni64\nf64\nf64\nf64\n\n\n\n\n0\n0.0\n1.345135\n17.504564\n\n\n0\n0.01\n0.178571\n34.642857\n\n\n0\n0.02\n4.35\n61.05\n\n\n0\n0.03\n16.25\n74.0\n\n\n0\n0.04\n0.03\n46.5\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n0.0\n12.5\n\n\n208\n6.6\n0.0\n17.7\n\n\n247\n3.31\n2.3\n11.5\n\n\n249\n1.69\n0.0\n8.5\n\n\n254\n1.02\n0.0\n6.0\n\n\n\n\n\n# print(f\"Time difference of {toc - tic} seconds\")\n\n\n\n\nq3 = (\n    nyc\n    $group_by(\"passenger_count\", \"trip_distance\")\n    $agg(\n        pl$mean(\"tip_amount\")$alias(\"mean_tip\"),\n        pl$mean(\"fare_amount\")$alias(\"mean_fare\")\n        )\n    $sort(\"passenger_count\", \"trip_distance\")\n)\n\ntic = Sys.time()\ndat3 = q3$collect()\ntoc = Sys.time()\n \ndat3\n\n\nshape: (25_569, 4)\n\n\n\npassenger_count\ntrip_distance\nmean_tip\nmean_fare\n\n\ni64\nf64\nf64\nf64\n\n\n\n\n0\n0.0\n1.345135\n17.504564\n\n\n0\n0.01\n0.178571\n34.642857\n\n\n0\n0.02\n4.35\n61.05\n\n\n0\n0.03\n16.25\n74.0\n\n\n0\n0.04\n0.03\n46.5\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n0.0\n12.5\n\n\n208\n6.6\n0.0\n17.7\n\n\n247\n3.31\n2.3\n11.5\n\n\n249\n1.69\n0.0\n8.5\n\n\n254\n1.02\n0.0\n6.0\n\n\n\n\n\ntoc - tic\n\nTime difference of 9.987634 secs\n\n\n\n\n\nAs an aside, if we didn’t care about column aliases (or sorting), then the previous query could be shortened to:\n\nPythonR\n\n\n(\n    nyc\n    .group_by([\"passenger_count\", \"trip_distance\"])\n    .agg(pl.col([\"tip_amount\", \"fare_amount\"]).mean())\n    .collect()\n)\n\n\n(\n    nyc\n    $group_by(\"passenger_count\", \"trip_distance\")\n    $agg(pl$col(\"tip_amount\", \"fare_amount\")$mean())\n    $collect()\n)",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#pivot-reshape",
    "href": "polars-rpy.html#pivot-reshape",
    "title": "Polars from Python and R",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\nIn polars, we have two distinct reshape methods:\n\npivot: =&gt; long to wide\nunpivot: =&gt; wide to long\n\nHere we’ll unpivot to go from wide to long and use the eager execution engine (i.e., on the dat3 DataFrame object that we’ve already computed) for expediency.\n\nPythonR\n\n\n\ndat3.unpivot(index = [\"passenger_count\", \"trip_distance\"])\n\n\nshape: (51_138, 4)\n\n\n\npassenger_count\ntrip_distance\nvariable\nvalue\n\n\ni64\nf64\nstr\nf64\n\n\n\n\n0\n0.0\n\"mean_tip\"\n1.345135\n\n\n0\n0.01\n\"mean_tip\"\n0.178571\n\n\n0\n0.02\n\"mean_tip\"\n4.35\n\n\n0\n0.03\n\"mean_tip\"\n16.25\n\n\n0\n0.04\n\"mean_tip\"\n0.03\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n\"mean_fare\"\n12.5\n\n\n208\n6.6\n\"mean_fare\"\n17.7\n\n\n247\n3.31\n\"mean_fare\"\n11.5\n\n\n249\n1.69\n\"mean_fare\"\n8.5\n\n\n254\n1.02\n\"mean_fare\"\n6.0\n\n\n\n\n\n\n\n\n\ndat3$unpivot(index = c(\"passenger_count\", \"trip_distance\"))\n\n\nshape: (51_138, 4)\n\n\n\npassenger_count\ntrip_distance\nvariable\nvalue\n\n\ni64\nf64\nstr\nf64\n\n\n\n\n0\n0.0\n\"mean_tip\"\n1.345135\n\n\n0\n0.01\n\"mean_tip\"\n0.178571\n\n\n0\n0.02\n\"mean_tip\"\n4.35\n\n\n0\n0.03\n\"mean_tip\"\n16.25\n\n\n0\n0.04\n\"mean_tip\"\n0.03\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n\"mean_fare\"\n12.5\n\n\n208\n6.6\n\"mean_fare\"\n17.7\n\n\n247\n3.31\n\"mean_fare\"\n11.5\n\n\n249\n1.69\n\"mean_fare\"\n8.5\n\n\n254\n1.02\n\"mean_fare\"\n6.0",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#joins-merges",
    "href": "polars-rpy.html#joins-merges",
    "title": "Polars from Python and R",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n\nPythonR\n\n\n\nmean_tips  = nyc.group_by(\"month\").agg(pl.col(\"tip_amount\").mean())\nmean_fares = nyc.group_by(\"month\").agg(pl.col(\"fare_amount\").mean())\n\n\n(\n    mean_tips\n    .join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\" # default is inner join\n    )\n    .collect()\n)\n\n\nshape: (12, 3)\n\n\n\nmonth\ntip_amount\nfare_amount\n\n\ni64\nf64\nf64\n\n\n\n\n4\n1.043167\n10.33549\n\n\n8\n1.079521\n10.49265\n\n\n7\n1.059312\n10.379943\n\n\n5\n1.078014\n10.585157\n\n\n1\n1.007817\n9.813488\n\n\n…\n…\n…\n\n\n11\n1.250903\n12.270138\n\n\n12\n1.237651\n12.313953\n\n\n2\n1.036874\n9.94264\n\n\n10\n1.281239\n12.501252\n\n\n6\n1.091082\n10.548651\n\n\n\n\n\n\n\n\n\nmean_tips  = nyc$group_by(\"month\")$agg(pl$col(\"tip_amount\")$mean())\nmean_fares = nyc$group_by(\"month\")$agg(pl$col(\"fare_amount\")$mean())\n\n\n(\n    mean_tips\n    $join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\"  # default is inner join\n    )\n    $collect()\n)\n\n\nshape: (12, 3)\n\n\n\nmonth\ntip_amount\nfare_amount\n\n\ni64\nf64\nf64\n\n\n\n\n4\n1.043167\n10.33549\n\n\n3\n1.056353\n10.223107\n\n\n7\n1.059312\n10.379943\n\n\n5\n1.078014\n10.585157\n\n\n2\n1.036874\n9.94264\n\n\n…\n…\n…\n\n\n12\n1.237651\n12.313953\n\n\n6\n1.091082\n10.548651\n\n\n9\n1.254601\n12.391198\n\n\n8\n1.079521\n10.49265\n\n\n10\n1.281239\n12.501252",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#appendix-alternate-interfaces",
    "href": "polars-rpy.html#appendix-alternate-interfaces",
    "title": "Polars from Python and R",
    "section": "Appendix: Alternate interfaces",
    "text": "Appendix: Alternate interfaces\nThe native polars API is not the only way to interface with the underlying computation engine. Here are two alternate approaches that you may prefer, especially if you don’t want to learn a new syntax.\n\nIbis (Python)\nThe great advantage of Ibis (like dbplyr) is that it supports multiple backends through an identical frontend. So, all of our syntax logic and workflow from the Ibis+DuckDB section carry over to an equivalent Ibis+Polars workflow too. All you need to do is change the connection type. For example:\n\nimport ibis\nimport ibis.selectors as s\nfrom ibis import _\n\n##! This next line is the only thing that's changed !##\ncon = ibis.polars.connect()\n\ncon.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n\n&lt;string&gt;:2: FutureWarning: `Backend.register` is deprecated as of v9.1; use the explicit `read_*` method for the filetype you are trying to read, e.g., read_parquet, read_csv, etc.\nDatabaseTable: nyc\n  vendor_name           string\n  pickup_datetime       timestamp(3)\n  dropoff_datetime      timestamp(3)\n  passenger_count       int64\n  trip_distance         float64\n  pickup_longitude      float64\n  pickup_latitude       float64\n  rate_code             string\n  store_and_fwd         string\n  dropoff_longitude     float64\n  dropoff_latitude      float64\n  payment_type          string\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  total_amount          float64\n  improvement_surcharge float64\n  congestion_surcharge  float64\n  pickup_location_id    int64\n  dropoff_location_id   int64\n\nnyc = con.table(\"nyc\")\n\n(\n  nyc\n  .group_by([\"passenger_count\"])\n  .agg(mean_tip = _.tip_amount.mean())\n  .to_polars()\n)\n\nshape: (18, 2)\n┌─────────────────┬──────────┐\n│ passenger_count ┆ mean_tip │\n│ ---             ┆ ---      │\n│ i64             ┆ f64      │\n╞═════════════════╪══════════╡\n│ 5               ┆ 1.102732 │\n│ 8               ┆ 0.350769 │\n│ 0               ┆ 0.862099 │\n│ 4               ┆ 0.844519 │\n│ 249             ┆ 0.0      │\n│ …               ┆ …        │\n│ 6               ┆ 1.128365 │\n│ 177             ┆ 1.0      │\n│ 3               ┆ 0.962949 │\n│ 254             ┆ 0.0      │\n│ 247             ┆ 2.3      │\n└─────────────────┴──────────┘\n\n\n\n\ntidypolars (R)\nThe R package tidypolars (link) provides the “tidyverse” syntax while using polars as backend. The syntax and workflow should thus be immediately familar to R users.\nIt’s important to note that tidypolars is solely focused on the translation work. This means that you still need to load the main polars library alongside it for the actual computation, as well as dplyr (and potentially tidyr) for function generics.\n\nlibrary(polars) ## Already loaded\nlibrary(tidypolars)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)\n\nnyc = scan_parquet_polars(\"nyc-taxi/**/*.parquet\")\n\nnyc |&gt; \n    summarise(mean_tip = mean(tip_amount), .by = passenger_count) |&gt;\n    compute()\n\n\nshape: (18, 2)\n\n\n\npassenger_count\nmean_tip\n\n\ni64\nf64\n\n\n\n\n6\n1.128365\n\n\n9\n0.8068\n\n\n0\n0.862099\n\n\n3\n0.962949\n\n\n66\n1.5\n\n\n…\n…\n\n\n8\n0.350769\n\n\n249\n0.0\n\n\n208\n0.0\n\n\n65\n0.0\n\n\n247\n2.3\n\n\n\n\n\n\nAside: Use collect() instead of compute() at the end if you would prefer to return a standard R data.frame instead of a Polars DataFrame.\nSee also polarssql (link) if you would like yet another “tidyverse”-esque alternative that works through DBI/d(b)plyr.",
    "crumbs": [
      "Polars from Python and R"
    ]
  }
]